[
  {
    "objectID": "content/01_journal/01_Machine_Learning_Fundamentals.html",
    "href": "content/01_journal/01_Machine_Learning_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "1 Challenge\n\n##----libraries----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\n#> Loading required package: PerformanceAnalytics\n#> Loading required package: xts\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> \n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attaching package: 'xts'\n#> \n#> The following objects are masked from 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attaching package: 'PerformanceAnalytics'\n#> \n#> The following object is masked from 'package:graphics':\n#> \n#>     legend\n#> \n#> Loading required package: quantmod\n#> Loading required package: TTR\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\nlibrary(broom)\nlibrary(umap)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n##----.read the data----\nsp_500_prices_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/1/sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/1/sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n##----.Step1 (Convert stock prices to a standardized format (daily returns)----\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>%\n  filter(date >= as.Date(\"2018-01-01\")) %>%\n  select(symbol, date, adjusted) %>%\n  group_by(symbol) %>%\n  mutate(lag_adjusted = lag(adjusted),\n         pct_return = (adjusted - lag_adjusted) / lag_adjusted) %>%\n  filter(!is.na(pct_return)) %>%\n  select(symbol, date, pct_return) \n  sp_500_daily_returns_tbl\n\n\n\n  \n\n\n##----.Step2 (Convert to User-Item Format)----\nstock_date_matrix_tbl <- sp_500_daily_returns_tbl %>%\n  select(symbol, date, pct_return) %>%\n  pivot_wider(names_from = date, values_from = pct_return, values_fill = 0) %>%\n  ungroup()\nstock_date_matrix_tbl\n\n\n\n  \n\n\n##----.step3 (Perform K-Means Clustering)----\n?kmeans\n\n#> starting httpd help server ... done\n\nkmeans_obj <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  kmeans(centers = 4, nstart = 20)\n#Get the tot.withinss using glance()\nglance(kmeans_obj)\n\n\n\n  \n\n\n##----.step4 (Find the optimal value of K)----\nkmeans_mapper <- function(center = 4) {\n  stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    kmeans(centers = center, nstart = 20)\n}\n\n4 %>% kmeans_mapper() %>% glance()\n\n\n\n  \n\n\nkmeans_mapped_tbl <- tibble(centers = 1:30) %>%\n  mutate(k_means = centers %>% map(kmeans_mapper)) %>%\n  mutate(glance  = k_means %>% map(glance))\n\n#> Warning: There was 1 warning in `mutate()`.\n#> ℹ In argument: `k_means = centers %>% map(kmeans_mapper)`.\n#> Caused by warning:\n#> ! did not converge in 10 iterations\n\nkmeans_mapped_tbl %>%\n  unnest(glance) %>%\n  select(centers, tot.withinss)\n\n\n\n  \n\n\n#Scree Plot\nkmeans_mapped_tbl %>%\n  unnest(glance) %>%\n  select(centers, tot.withinss) %>%\n  ggplot(aes(centers, tot.withinss)) +\n  geom_point(color = \"#2DC6D6\", size = 4) +\n  geom_line(color = \"#2DC6D6\", size = 1) +\n  ggrepel::geom_label_repel(aes(label = centers), color = \"#2DC6D6\",max.overlaps = 30) + \n  labs(title = \"Scree Plot\",\n       subtitle = \"Measures the distance each of the symbols are from the closes K-Means center\",\n       caption = \"Conclusion: Based on the Scree Plot, We can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K.\")\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n##----.step5 (Apply UMAP)----\n?umap\numap_results <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  umap()\n\numap_results_tbl <- umap_results$layout %>%\n  as_tibble(.name_repair = \"unique\") %>% \n  set_names(c(\"x\", \"y\")) %>%\n  bind_cols(\n    stock_date_matrix_tbl %>% select(symbol)\n  )\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\numap_results_tbl %>%\n  ggplot(aes(x, y)) +\n  geom_point(size=0.5) + \n  geom_label_repel(aes(label = \"UMAP Projection\"), size = 3)\n\n#> Warning: ggrepel: 494 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps\n\n\n\n\n\n\n\n#----.step6 (Combine K-Means and UMAP)----\nkmeans_obj <- kmeans_mapped_tbl %>%\n  pull(k_means) %>%\n  pluck(10)\n\nkmeans_clusters_tbl <- kmeans_obj %>% \n  augment(stock_date_matrix_tbl) %>%\n  select(symbol, .cluster)\numap_kmeans_results_tbl <- umap_results_tbl %>%\n  left_join(kmeans_clusters_tbl, by = \"symbol\") %>%\n  left_join(sp_500_index_tbl %>% select(symbol, company, sector), by = \"symbol\")\n\numap_kmeans_results_tbl %>%\n  mutate(label_text = str_glue(\"Customer: {symbol}\n                               Cluster: {.cluster}\")) %>%\n  \n  ggplot(aes(x, y, color = .cluster)) +\n  geom_point(size=0.5) +\n  geom_label_repel(aes(label = label_text), size = 2, fill = \"blue\", color = \"white\", max.overlaps = 30) +\n  scale_color_manual(values = palette_light() %>% rep(3)) +\n  labs(title = \"2D Projection\",\n       subtitle = \"UMAP 2D Projection with K-Means Cluster Assignment\") +\n  theme(legend.position = \"none\")\n\n#> Warning: ggrepel: 175 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps"
  },
  {
    "objectID": "content/01_journal/02_Supervised_ML_Regression.html",
    "href": "content/01_journal/02_Supervised_ML_Regression.html",
    "title": "Supervised ML Regression",
    "section": "",
    "text": "1 Challenge\n\n##----.libraries----\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(workflows)\nlibrary(broom.mixed)\nlibrary(parsnip)\nlibrary(recipes)\n\n#> \n#> Attaching package: 'recipes'\n#> \n#> The following object is masked from 'package:stringr':\n#> \n#>     fixed\n#> \n#> The following object is masked from 'package:stats':\n#> \n#>     step\n\nlibrary(rsample)\nlibrary(yardstick)\n\n#> \n#> Attaching package: 'yardstick'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     spec\n\nlibrary(rpart.plot)\n\n#> Loading required package: rpart\n\n##----.Read the data----\nbike_features_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/2/bike_features_tbl.rds\")\n\n\nbike_features_tbl <- bike_features_tbl\n\n##----.Create a recipe----\nbike_recipe <- recipe(price ~ category_2 + frame_material, data = bike_features_tbl) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_predictors())\n\nbike_recipe_prep <- prep(bike_recipe)\n\n##----.Split the data into training and test sets----\nset.seed(1113)\nsplit_obj <- initial_split(bike_features_tbl, prop = 0.8, strata = \"category_2\")\ntrain_tbl <- training(split_obj)\ntest_tbl <- testing(split_obj)\n\n##----.Create a workflow for Model 01----\nwf_model_01 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n##----.Fit Model 01 using the workflow----\nmodel_01_linear_lm_simple <- fit(wf_model_01, data = train_tbl)\n\n##----.Make predictions on the test data for Model 01----\npredictions_model_01 <- predict(model_01_linear_lm_simple, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Calculate the RMSE manually for Model 01----\nrmse_value_model_01 <- sqrt(mean((predictions_model_01$price - predictions_model_01$.pred)^2))\n\n#----.View the calculated RMSE for Model 01----\nprint(rmse_value_model_01)\n\n#> [1] NA\n\n##----.Extract and tidy the coefficients for Model 01----\ncoefs_model_01 <- broom.mixed::tidy(model_01_linear_lm_simple$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n##----.Plot the feature importance for Model 01----\nggplot(coefs_model_01, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  geom_label(aes(label = scales::dollar(coefs_model_01$estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n             size = 3, fill = \"#272A36\", color = \"white\", hjust = 0) +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 01: Simple lm Model\")\n\n#> Warning: Use of `coefs_model_01$estimate` is discouraged.\n#> ℹ Use `estimate` instead.\n\n\n\n\n\n\n\n##----.Define the helper function to calculate metrics----\ncalc_metrics <- function(model, new_data = test_tbl) {\n  model %>%\n    predict(new_data = new_data) %>%\n    bind_cols(new_data %>% select(price)) %>%\n    yardstick::metrics(truth = price, estimate = .pred)\n}\n\n##----.Calculate the metrics using the helper function for Model 01----\nmetrics_model_01 <- model_01_linear_lm_simple %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Print the calculated metrics for Model 01----\nprint(metrics_model_01)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n##----.Create a workflow for Model 02----\nwf_model_02 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n##----.Fit Model 02 using the workflow----\nmodel_02_linear_lm_complex <- fit(wf_model_02, data = train_tbl)\n\n##----.Make predictions on the test data for Model 02----\npredictions_model_02 <- predict(model_02_linear_lm_complex, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Calculate the RMSE manually for Model 02----\nrmse_value_model_02 <- sqrt(mean((predictions_model_02$price - predictions_model_02$.pred)^2))\n\n##----.View the calculated RMSE for Model 02----\nprint(rmse_value_model_02)\n\n#> [1] NA\n\n##----.Extract and tidy the coefficients for Model 02----\ncoefs_model_02 <- tidy(model_02_linear_lm_complex$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n##----.Plot the feature importance for Model 02----\nggplot(coefs_model_02, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 02: Complex lm Model\")\n\n\n\n\n\n\n##----.Calculate the metrics using the helper function for Model 02----\nmetrics_model_02 <- model_02_linear_lm_complex %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Print the calculated metrics for Model 02----\nprint(metrics_model_02)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n##----.TESTING THE ALGORITHMS OUT----\ng1 <- bike_features_tbl %>% \n  mutate(category_2 = as.factor(category_2) %>% \n           fct_reorder(price)) %>% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Unit Price for Each Model\",\n    y = \"\", x = \"Category 2\"\n  )\n\n##----.Print the plot----\nprint(g1)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n\n\n\n\n\n##----.NEW MODEL----\n\nnew_cross_country <- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Cross-Country\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \n\nnew_cross_country\n\n\n\n  \n\n\n##----.Linear Methods----\n\n# Iteration\nmodels_tbl <- tibble(\n  model_id = str_c(\"Model 0\", 1:2),\n  model = list(\n    model_01_linear_lm_simple,\n    model_02_linear_lm_complex\n  )\n)\n\nmodels_tbl\n\n\n\n  \n\n\n##----.Add Predictions----\npredictions_new_cross_country_tbl <- models_tbl %>%\n  mutate(predictions = map(model, predict, new_data = new_cross_country)) %>%\n  unnest(predictions) %>%\n  mutate(category_2 = \"Cross-Country\") %>%\n  left_join(new_cross_country, by = \"category_2\")\n\npredictions_new_cross_country_tbl\n\n\n\n  \n\n\n##----.Update plot----\ng2 <- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_cross_country_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 3,\n                           data = predictions_new_cross_country_tbl)\nprint(g2)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped."
  },
  {
    "objectID": "content/01_journal/03_Automated_Machine_Learning_with_H20_(I).html",
    "href": "content/01_journal/03_Automated_Machine_Learning_with_H20_(I).html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "1 Compensation Features:\nWhat can you deduce about the interaction between Monthly Income and Attrition?\na)Those that are leaving the company have a higher Monthly Income\nb)That those are staying have a lower Monthly Income\nc)Those that are leaving have a lower Monthly Income\nd)It’s difficult to deduce anything based on the visualization\n\n2 Compensation Features:\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition?\na)Those that are leaving the company have a higher Percent Salary Hike\nb)Those that are staying have a lower Percent Salary Hike\nc)Those that are leaving have lower Percent Salary Hike\nd)It’s difficult to deduce anything based on the visualization\n\n3 Compensation Features:\nWhat can you deduce about the interaction between Stock Option Level and Attrition?\na)Those that are leaving the company have a higher stock option level\nb)Those that are staying have a higher stock option level\nc)It’s difficult to deduce anything based on the visualization\n\n4 Survey Results\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition?\na)A higher proportion of those leaving have a low environment satisfaction level\nb)A higher proportion of those leaving have a high environment satisfaction level\nc)It’s difficult to deduce anything based on the visualization\n\n5 Survey Results\nWhat can you deduce about the interaction between Work Life Balance and Attrition?\na)Those that are leaving have higher density of 2’s and 3’s\nb)Those that are staying have a higher density of 2’s and 3’s\nc)Those that are staying have a lower density of 2’s and 3’s\nd)It’s difficult to deduce anything based on the visualization\n\n6 Performance Data\nWhat Can you deduce about the interaction between Job Involvement and Attrition?\na)Those that are leaving have a lower density of 3’s and 4’s\nb)Those that are leaving have a lower density of 1’s and 2’s\nc)Those that are staying have a lower density of 2’s and 3’s\nd)It’s difficult to deduce anything based on the visualization\n\n7 Work-Life Features\nWhat can you deduce about the interaction between Over Time and Attrition?\na)The proportion of those leaving that are working Over Time are high compared to those that are not leaving\nb)The proportion of those staying that are working Over Time are high compared to those that are not staying\n\n8 Training and Education\nWhat can you deduce about the interaction between Training Times Last Year and Attrition\na)People that leave tend to have more annual trainings\nb)People that leave tend to have less annual trainings\nc)It’s difficult to deduce anything based on the visualization\n\n9 Time-Based Features\nWhat can you deduce about the interaction between Years At Company and Attrition\na)People that leave tend to have more working years at the company\nb)People that leave tend to have less working years at the company\nc)It’s difficult to deduce anything based on the visualization\n\n10 Time-Based Features\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition?\na)Those that are leaving have more years since last promotion than those that are staying\nb)Those that are leaving have fewer years since last promotion than those that are staying\nc)It’s difficult to deduce anything based on the visualization\n\n11 code\n\n##----.libraries----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\n\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\n##----.Load data-----\nemployee_attrition_tbl <- read_csv(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndept_job_role_tbl <- employee_attrition_tbl %>%\nselect(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\n\n##----.Function to convert counts to percentages---- \ncount_to_pct <- function(data, ..., col = n) {\n  \n  # capture the dots\n  grouping_vars_expr <- quos(...)\n  col_expr <- enquo(col)\n  \n  ret <- data %>%\n    group_by(!!! grouping_vars_expr) %>%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%\n    ungroup()\n  \n  return(ret)\n  \n}\n\n\n##----.Develop KPI function----\n\nassess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {\n  \n  attrition_col_expr <- enquo(attrition_col)\n  \n  data %>%\n    \n    # Use parenthesis () to give tidy eval evaluation priority\n    filter((!! attrition_col_expr) %in% attrition_value) %>%\n    arrange(desc(pct)) %>%\n    mutate(\n      # Function inputs in numeric format\n      above_industry_avg = case_when(\n        pct > baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n    )\n  \n}\n\n\n##----.Function to calculate attrition cost----\ncalculate_attrition_cost <- function(\n    \n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  \n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  \n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50\n  \n) {\n  \n  # Direct Costs\n  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  \n  # Lost Productivity Costs\n  productivity_cost <- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  \n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open\n  \n  # Estimated Turnover Per Employee\n  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction\n  \n  # Total Cost of Employee Turnover\n  total_cost <- n * cost_per_employee\n  \n  return(total_cost)\n  \n}\n\n\n##----.Function to plot attrition----\nplot_attrition <- function(data, \n                           ..., \n                           .value,\n                           fct_reorder = TRUE,\n                           fct_rev     = FALSE,\n                           include_lbl = TRUE,\n                           color       = \"#2dc6d6\",\n                           units       = c(\"0\", \"K\", \"M\")) {\n  \n  ### Inputs\n  group_vars_expr   <- quos(...)\n  \n  # If the user does not supply anything, \n  # this takes the first column of the supplied data\n  if (length(group_vars_expr) == 0) {\n    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))\n  }\n  \n  value_expr <- enquo(.value)\n  \n  units_val  <- switch(units[[1]],\n                       \"M\" = 1e6,\n                       \"K\" = 1e3,\n                       \"0\" = 1)\n  if (units[[1]] == \"0\") units <- \"\"\n  \n  # Data Manipulation\n  # This is a so called Function Factory (a function that produces a function)\n  usd <- scales::dollar_format(prefix = \"$\", largest_with_cents = 1e3)\n  \n  # Create the axis labels and values for the plot\n  data_manipulated <- data %>%\n    mutate(name = str_c(!!! group_vars_expr, sep = \": \") %>% as_factor()) %>%\n    mutate(value_text = str_c(usd(!! value_expr / units_val),\n                              units[[1]], sep = \"\"))\n  \n  \n  # Order the labels on the y-axis according to the input\n  if (fct_reorder) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%\n      arrange(name)\n  }\n  \n  if (fct_rev) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_rev(name)) %>%\n      arrange(name)\n  }\n  \n\n##----.Visualization----\n  \n  g <- data_manipulated %>%\n    \n    # \"name\" is a column name generated by our function internally as part of the data manipulation task\n    ggplot(aes(x = (!! value_expr), y = name)) +\n    geom_segment(aes(xend = 0, yend = name), color = color) +\n    geom_point(aes(size = !! value_expr), color = color) +\n    scale_x_continuous(labels = scales::dollar) +\n    scale_size(range = c(3, 5)) +\n    theme(legend.position = \"none\")\n  \n  # Plot labels if TRUE\n  if (include_lbl) {\n    g <- g +\n      geom_label(aes(label = value_text, size = !! value_expr),\n                 hjust = \"inward\", color = color)\n  }\n  \n  return(g)\n}\n\n##----.calculations----\nattrition_cost_df <- dept_job_role_tbl %>%\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole)  %>%  \n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %>%\n  # Select columnns\n  plot_attrition(Department, JobRole, .value = cost_of_attrition,\n                 units = \"M\") +\n  labs(\n    title = \"Estimated Cost of Attrition by Job Role\",\n    x = \"Cost of Attrition\",\n    subtitle = \"Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost\"\n  )\n  \n\n\n##----.Custom plotting function----\n\ndata <- employee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome)\n\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n  \n  color_expr <- enquo(color)\n  \n  if (rlang::quo_is_null(color_expr)) {\n    \n    g <- data %>%\n      ggpairs(lower = \"blank\") \n    \n  } else {\n    \n    color_name <- quo_name(color_expr)\n    \n    g <- data %>%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")\n  }\n  \n  return(g)\n  \n}\n\nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(color = Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n##----.Explore Features by Category----\n\n#   1. Descriptive features: age, gender, marital status \nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   2. Employment features: department, job role, job level\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %>%\n  plot_ggpairs(Attrition) \n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n\n\n\n\n\n#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   4. Survey Results: Satisfaction level, WorkLifeBalance \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   5. Performance Data: Job Involvement, Performance Rating\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   6. Work-Life Features \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   7. Training and Education \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   8. Time-Based Features: Years at company, years in current role\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"years\")) %>%\n  plot_ggpairs(Attrition)"
  },
  {
    "objectID": "content/01_journal/04_Automated_Machine_Learning_with_H20_(II).html",
    "href": "content/01_journal/04_Automated_Machine_Learning_with_H20_(II).html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "1 challenge\n\n##----.libraries----\nlibrary(h2o)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(purrr)\n\n##----.Initialize H2O----\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         5 minutes 1 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_mosta_svg245 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   2.98 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n##----.Load the training, validation, and test datasets----\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.response and predictor variables----\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n##----.AutoML----\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 12:22:18.911: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 12:22:18.911: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.leaderboard----\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                  model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_3_20230607_122218 0.9554813 0.1646200\n#> 2    StackedEnsemble_AllModels_1_AutoML_3_20230607_122218 0.9551619 0.1650368\n#> 3 StackedEnsemble_BestOfFamily_3_AutoML_3_20230607_122218 0.9545483 0.1664879\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_3_20230607_122218 0.9538884 0.1677040\n#> 5                          GBM_4_AutoML_3_20230607_122218 0.9531997 0.1689092\n#> 6                          GBM_3_AutoML_3_20230607_122218 0.9528312 0.1706504\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7602304            0.1333442 0.2216200 0.04911544\n#> 2 0.7585778            0.1275209 0.2219367 0.04925589\n#> 3 0.7564890            0.1367932 0.2223929 0.04945860\n#> 4 0.7523904            0.1443508 0.2232434 0.04983762\n#> 5 0.7517860            0.1357064 0.2242145 0.05027215\n#> 6 0.7442681            0.1396240 0.2255727 0.05088305\n#> \n#> [17 rows x 7 columns]\n\n##----leader model----\nleader_model <- automl_models_h2o@leader\nleader_model\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_2_AutoML_3_20230607_122218 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)              4/9\n#> 3           # GBM base models (used / total)              3/5\n#> 4           # DRF base models (used / total)              1/2\n#> 5           # GLM base models (used / total)              0/1\n#> 6  # DeepLearning base models (used / total)              0/1\n#> 7                      Metalearner algorithm              GLM\n#> 8         Metalearner fold assignment scheme           Random\n#> 9                         Metalearner nfolds                5\n#> 10                   Metalearner fold_column               NA\n#> 11        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02576233\n#> RMSE:  0.1605065\n#> LogLoss:  0.09475967\n#> Mean Per-Class Error:  0.08100955\n#> AUC:  0.9899324\n#> AUCPR:  0.9388804\n#> Gini:  0.9798649\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8753  160 0.017951   =160/8913\n#> Yes     170 1010 0.144068   =170/1180\n#> Totals 8923 1170 0.032696  =330/10093\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.428451    0.859574 173\n#> 2                       max f2  0.261137    0.890610 225\n#> 3                 max f0point5  0.593135    0.886546 131\n#> 4                 max accuracy  0.486267    0.967601 158\n#> 5                max precision  0.988006    1.000000   0\n#> 6                   max recall  0.017611    1.000000 361\n#> 7              max specificity  0.988006    1.000000   0\n#> 8             max absolute_mcc  0.428451    0.841083 173\n#> 9   max min_per_class_accuracy  0.217375    0.948053 241\n#> 10 max mean_per_class_accuracy  0.173691    0.950907 260\n#> 11                     max tns  0.988006 8913.000000   0\n#> 12                     max fns  0.988006 1179.000000   0\n#> 13                     max fps  0.000218 8913.000000 399\n#> 14                     max tps  0.017611 1180.000000 361\n#> 15                     max tnr  0.988006    1.000000   0\n#> 16                     max fnr  0.988006    0.999153   0\n#> 17                     max fpr  0.000218    1.000000 399\n#> 18                     max tpr  0.017611    1.000000 361\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.02616927\n#> RMSE:  0.1617692\n#> LogLoss:  0.09670113\n#> Mean Per-Class Error:  0.08181059\n#> AUC:  0.9894601\n#> AUCPR:  0.940179\n#> Gini:  0.9789201\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error        Rate\n#> No     16485  302 0.017990  =302/16787\n#> Yes      330 1936 0.145631   =330/2266\n#> Totals 16815 2238 0.033171  =632/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.428771     0.859680 179\n#> 2                       max f2  0.265840     0.887850 231\n#> 3                 max f0point5  0.602916     0.892516 130\n#> 4                 max accuracy  0.484175     0.967039 163\n#> 5                max precision  0.988388     1.000000   0\n#> 6                   max recall  0.018195     1.000000 366\n#> 7              max specificity  0.988388     1.000000   0\n#> 8             max absolute_mcc  0.428771     0.840894 179\n#> 9   max min_per_class_accuracy  0.217630     0.945434 249\n#> 10 max mean_per_class_accuracy  0.199183     0.948316 256\n#> 11                     max tns  0.988388 16787.000000   0\n#> 12                     max fns  0.988388  2259.000000   0\n#> 13                     max fps  0.000150 16787.000000 399\n#> 14                     max tps  0.018195  2266.000000 366\n#> 15                     max tnr  0.988388     1.000000   0\n#> 16                     max fnr  0.988388     0.996911   0\n#> 17                     max fpr  0.000150     1.000000 399\n#> 18                     max tpr  0.018195     1.000000 366\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04911544\n#> RMSE:  0.22162\n#> LogLoss:  0.16462\n#> Mean Per-Class Error:  0.1333442\n#> AUC:  0.9554813\n#> AUCPR:  0.7602304\n#> Gini:  0.9109626\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     15829  958 0.057068   =958/16787\n#> Yes      475 1791 0.209620    =475/2266\n#> Totals 16304 2749 0.075211  =1433/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.280932     0.714257 227\n#> 2                       max f2  0.153478     0.786508 276\n#> 3                 max f0point5  0.590009     0.738244 132\n#> 4                 max accuracy  0.530186     0.932399 150\n#> 5                max precision  0.992470     1.000000   0\n#> 6                   max recall  0.000389     1.000000 399\n#> 7              max specificity  0.992470     1.000000   0\n#> 8             max absolute_mcc  0.280932     0.675570 227\n#> 9   max min_per_class_accuracy  0.123123     0.892321 289\n#> 10 max mean_per_class_accuracy  0.093141     0.894898 306\n#> 11                     max tns  0.992470 16787.000000   0\n#> 12                     max fns  0.992470  2264.000000   0\n#> 13                     max fps  0.000389 16787.000000 399\n#> 14                     max tps  0.000389  2266.000000 399\n#> 15                     max tnr  0.992470     1.000000   0\n#> 16                     max fnr  0.992470     0.999117   0\n#> 17                     max fpr  0.000389     1.000000 399\n#> 18                     max tpr  0.000389     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.928883  0.006950   0.933264   0.928211   0.918626   0.937002\n#> auc         0.955693  0.003417   0.954147   0.957676   0.951477   0.960364\n#> err         0.071117  0.006950   0.066736   0.071789   0.081374   0.062998\n#> err_count 270.800000 24.118458 255.000000 270.000000 308.000000 245.000000\n#> f0point5    0.695162  0.022954   0.698467   0.692220   0.670122   0.731547\n#>           cv_5_valid\n#> accuracy    0.927311\n#> auc         0.954799\n#> err         0.072689\n#> err_count 276.000000\n#> f0point5    0.683453\n#> \n#> ---\n#>                          mean        sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.679658  0.032212    0.684760    0.669742    0.646552\n#> r2                   0.531421  0.018991    0.522256    0.539250    0.506773\n#> recall               0.768083  0.026319    0.759259    0.799559    0.784519\n#> residual_deviance 1254.122200 58.529808 1243.656400 1226.594500 1354.737800\n#> rmse                 0.221547  0.007137    0.218876    0.221144    0.233286\n#> specificity          0.950492  0.010002    0.955444    0.945872    0.938010\n#>                    cv_4_valid  cv_5_valid\n#> precision            0.731868    0.665370\n#> r2                   0.557685    0.531143\n#> recall               0.730263    0.766816\n#> residual_deviance 1203.428600 1242.193500\n#> rmse                 0.213967    0.220462\n#> specificity          0.964463    0.948672\n\n##----.Predict using the leader model----\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n##----.Set the directory path to save the leader model----\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n##----.Save the leader model----\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_3_20230607_122218\""
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/01_journal/06_Explaining_Black-Box_Models_With_LIMEs.html",
    "href": "content/01_journal/06_Explaining_Black-Box_Models_With_LIMEs.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "Hi! In this website you will see the codes for the course “Business Decisions with Machine Learning”! :)"
  }
]