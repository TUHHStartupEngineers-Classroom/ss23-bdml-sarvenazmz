[
  {
    "objectID": "content/01_journal/01_Machine_Learning_Fundamentals.html",
    "href": "content/01_journal/01_Machine_Learning_Fundamentals.html",
    "title": "Machine Learning Fundamentals",
    "section": "",
    "text": "1 Challenge\n\n##----libraries----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(tidyquant)\n\n#> Loading required package: PerformanceAnalytics\n#> Loading required package: xts\n#> Loading required package: zoo\n#> \n#> Attaching package: 'zoo'\n#> \n#> The following objects are masked from 'package:base':\n#> \n#>     as.Date, as.Date.numeric\n#> \n#> \n#> ######################### Warning from 'xts' package ##########################\n#> #                                                                             #\n#> # The dplyr lag() function breaks how base R's lag() function is supposed to  #\n#> # work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n#> # source() into this session won't work correctly.                            #\n#> #                                                                             #\n#> # Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n#> # conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n#> # dplyr from breaking base R's lag() function.                                #\n#> #                                                                             #\n#> # Code in packages is not affected. It's protected by R's namespace mechanism #\n#> # Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#> #                                                                             #\n#> ###############################################################################\n#> \n#> Attaching package: 'xts'\n#> \n#> The following objects are masked from 'package:dplyr':\n#> \n#>     first, last\n#> \n#> \n#> Attaching package: 'PerformanceAnalytics'\n#> \n#> The following object is masked from 'package:graphics':\n#> \n#>     legend\n#> \n#> Loading required package: quantmod\n#> Loading required package: TTR\n#> Registered S3 method overwritten by 'quantmod':\n#>   method            from\n#>   as.zoo.data.frame zoo\n\nlibrary(broom)\nlibrary(umap)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(ggplot2)\nlibrary(ggrepel)\n\n##----.read the data----\nsp_500_prices_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/1/sp_500_prices_tbl.rds\")\nsp_500_prices_tbl\n\n\n\n  \n\n\nsp_500_index_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/1/sp_500_index_tbl.rds\")\nsp_500_index_tbl\n\n\n\n  \n\n\n##----.Step1 (Convert stock prices to a standardized format (daily returns)----\nsp_500_daily_returns_tbl <- sp_500_prices_tbl %>%\n  filter(date >= as.Date(\"2018-01-01\")) %>%\n  select(symbol, date, adjusted) %>%\n  group_by(symbol) %>%\n  mutate(lag_adjusted = lag(adjusted),\n         pct_return = (adjusted - lag_adjusted) / lag_adjusted) %>%\n  filter(!is.na(pct_return)) %>%\n  select(symbol, date, pct_return) \n  sp_500_daily_returns_tbl\n\n\n\n  \n\n\n##----.Step2 (Convert to User-Item Format)----\nstock_date_matrix_tbl <- sp_500_daily_returns_tbl %>%\n  select(symbol, date, pct_return) %>%\n  pivot_wider(names_from = date, values_from = pct_return, values_fill = 0) %>%\n  ungroup()\nstock_date_matrix_tbl\n\n\n\n  \n\n\n##----.step3 (Perform K-Means Clustering)----\n?kmeans\n\n#> starting httpd help server ... done\n\nkmeans_obj <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  kmeans(centers = 4, nstart = 20)\n#Get the tot.withinss using glance()\nglance(kmeans_obj)\n\n\n\n  \n\n\n##----.step4 (Find the optimal value of K)----\nkmeans_mapper <- function(center = 4) {\n  stock_date_matrix_tbl %>%\n    select(-symbol) %>%\n    kmeans(centers = center, nstart = 20)\n}\n\n4 %>% kmeans_mapper() %>% glance()\n\n\n\n  \n\n\nkmeans_mapped_tbl <- tibble(centers = 1:30) %>%\n  mutate(k_means = centers %>% map(kmeans_mapper)) %>%\n  mutate(glance  = k_means %>% map(glance))\n\n#> Warning: There was 1 warning in `mutate()`.\n#> ℹ In argument: `k_means = centers %>% map(kmeans_mapper)`.\n#> Caused by warning:\n#> ! did not converge in 10 iterations\n\nkmeans_mapped_tbl %>%\n  unnest(glance) %>%\n  select(centers, tot.withinss)\n\n\n\n  \n\n\n#Scree Plot\nkmeans_mapped_tbl %>%\n  unnest(glance) %>%\n  select(centers, tot.withinss) %>%\n  ggplot(aes(centers, tot.withinss)) +\n  geom_point(color = \"#2DC6D6\", size = 4) +\n  geom_line(color = \"#2DC6D6\", size = 1) +\n  ggrepel::geom_label_repel(aes(label = centers), color = \"#2DC6D6\",max.overlaps = 30) + \n  labs(title = \"Scree Plot\",\n       subtitle = \"Measures the distance each of the symbols are from the closes K-Means center\",\n       caption = \"Conclusion: Based on the Scree Plot, We can see that the Scree Plot becomes linear (constant rate of change) between 5 and 10 centers for K.\")\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n##----.step5 (Apply UMAP)----\n?umap\numap_results <- stock_date_matrix_tbl %>%\n  select(-symbol) %>%\n  umap()\n\numap_results_tbl <- umap_results$layout %>%\n  as_tibble(.name_repair = \"unique\") %>% \n  set_names(c(\"x\", \"y\")) %>%\n  bind_cols(\n    stock_date_matrix_tbl %>% select(symbol)\n  )\n\n#> New names:\n#> • `` -> `...1`\n#> • `` -> `...2`\n\numap_results_tbl %>%\n  ggplot(aes(x, y)) +\n  geom_point(size=0.5) + \n  geom_label_repel(aes(label = \"UMAP Projection\"), size = 3)\n\n#> Warning: ggrepel: 494 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps\n\n\n\n\n\n\n\n#----.step6 (Combine K-Means and UMAP)----\nkmeans_obj <- kmeans_mapped_tbl %>%\n  pull(k_means) %>%\n  pluck(10)\n\nkmeans_clusters_tbl <- kmeans_obj %>% \n  augment(stock_date_matrix_tbl) %>%\n  select(symbol, .cluster)\numap_kmeans_results_tbl <- umap_results_tbl %>%\n  left_join(kmeans_clusters_tbl, by = \"symbol\") %>%\n  left_join(sp_500_index_tbl %>% select(symbol, company, sector), by = \"symbol\")\n\numap_kmeans_results_tbl %>%\n  mutate(label_text = str_glue(\"Customer: {symbol}\n                               Cluster: {.cluster}\")) %>%\n  \n  ggplot(aes(x, y, color = .cluster)) +\n  geom_point(size=0.5) +\n  geom_label_repel(aes(label = label_text), size = 2, fill = \"blue\", color = \"white\", max.overlaps = 30) +\n  scale_color_manual(values = palette_light() %>% rep(3)) +\n  labs(title = \"2D Projection\",\n       subtitle = \"UMAP 2D Projection with K-Means Cluster Assignment\") +\n  theme(legend.position = \"none\")\n\n#> Warning: ggrepel: 175 unlabeled data points (too many overlaps). Consider\n#> increasing max.overlaps"
  },
  {
    "objectID": "content/01_journal/02_Supervised_ML_Regression.html",
    "href": "content/01_journal/02_Supervised_ML_Regression.html",
    "title": "Supervised ML Regression",
    "section": "",
    "text": "1 Challenge\n\n##----.libraries----\nlibrary(ggplot2)\nlibrary(ggrepel)\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ lubridate 1.9.2     ✔ tibble    3.2.1\n#> ✔ purrr     1.0.1     ✔ tidyr     1.3.0\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(workflows)\nlibrary(broom.mixed)\nlibrary(parsnip)\nlibrary(recipes)\n\n#> \n#> Attaching package: 'recipes'\n#> \n#> The following object is masked from 'package:stringr':\n#> \n#>     fixed\n#> \n#> The following object is masked from 'package:stats':\n#> \n#>     step\n\nlibrary(rsample)\nlibrary(yardstick)\n\n#> \n#> Attaching package: 'yardstick'\n#> \n#> The following object is masked from 'package:readr':\n#> \n#>     spec\n\nlibrary(rpart.plot)\n\n#> Loading required package: rpart\n\n##----.Read the data----\nbike_features_tbl <- readRDS(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/2/bike_features_tbl.rds\")\n\n\nbike_features_tbl <- bike_features_tbl\n\n##----.Create a recipe----\nbike_recipe <- recipe(price ~ category_2 + frame_material, data = bike_features_tbl) %>%\n  step_dummy(all_nominal(), -all_outcomes()) %>%\n  step_zv(all_predictors())\n\nbike_recipe_prep <- prep(bike_recipe)\n\n##----.Split the data into training and test sets----\nset.seed(1113)\nsplit_obj <- initial_split(bike_features_tbl, prop = 0.8, strata = \"category_2\")\ntrain_tbl <- training(split_obj)\ntest_tbl <- testing(split_obj)\n\n##----.Create a workflow for Model 01----\nwf_model_01 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n##----.Fit Model 01 using the workflow----\nmodel_01_linear_lm_simple <- fit(wf_model_01, data = train_tbl)\n\n##----.Make predictions on the test data for Model 01----\npredictions_model_01 <- predict(model_01_linear_lm_simple, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Calculate the RMSE manually for Model 01----\nrmse_value_model_01 <- sqrt(mean((predictions_model_01$price - predictions_model_01$.pred)^2))\n\n#----.View the calculated RMSE for Model 01----\nprint(rmse_value_model_01)\n\n#> [1] NA\n\n##----.Extract and tidy the coefficients for Model 01----\ncoefs_model_01 <- broom.mixed::tidy(model_01_linear_lm_simple$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n##----.Plot the feature importance for Model 01----\nggplot(coefs_model_01, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  geom_label(aes(label = scales::dollar(coefs_model_01$estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n             size = 3, fill = \"#272A36\", color = \"white\", hjust = 0) +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 01: Simple lm Model\")\n\n#> Warning: Use of `coefs_model_01$estimate` is discouraged.\n#> ℹ Use `estimate` instead.\n\n\n\n\n\n\n\n##----.Define the helper function to calculate metrics----\ncalc_metrics <- function(model, new_data = test_tbl) {\n  model %>%\n    predict(new_data = new_data) %>%\n    bind_cols(new_data %>% select(price)) %>%\n    yardstick::metrics(truth = price, estimate = .pred)\n}\n\n##----.Calculate the metrics using the helper function for Model 01----\nmetrics_model_01 <- model_01_linear_lm_simple %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Print the calculated metrics for Model 01----\nprint(metrics_model_01)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n##----.Create a workflow for Model 02----\nwf_model_02 <- workflow() %>%\n  add_model(linear_reg(mode = \"regression\") %>% set_engine(\"lm\")) %>%\n  add_recipe(bike_recipe_prep)\n\n##----.Fit Model 02 using the workflow----\nmodel_02_linear_lm_complex <- fit(wf_model_02, data = train_tbl)\n\n##----.Make predictions on the test data for Model 02----\npredictions_model_02 <- predict(model_02_linear_lm_complex, new_data = test_tbl) %>%\n  bind_cols(data.frame(price = test_tbl$price, category_2 = as.character(test_tbl$category_2)))\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Calculate the RMSE manually for Model 02----\nrmse_value_model_02 <- sqrt(mean((predictions_model_02$price - predictions_model_02$.pred)^2))\n\n##----.View the calculated RMSE for Model 02----\nprint(rmse_value_model_02)\n\n#> [1] NA\n\n##----.Extract and tidy the coefficients for Model 02----\ncoefs_model_02 <- tidy(model_02_linear_lm_complex$fit$fit) %>%\n  arrange(p.value) %>%\n  mutate(term = as_factor(term) %>% fct_rev())\n\n##----.Plot the feature importance for Model 02----\nggplot(coefs_model_02, aes(x = estimate, y = term)) +\n  geom_point(color = \"#2dc6d6\", size = 3) +\n  ggrepel::geom_label_repel(aes(label = scales::dollar(estimate, accuracy = 1, suffix = \" €\", prefix = \"\")),\n                            size = 4, fill = \"#272A36\", color = \"white\") +\n  scale_x_continuous(labels = scales::dollar_format(suffix = \" €\", prefix = \"\")) +\n  labs(title = \"Linear Regression: Feature Importance\",\n       subtitle = \"Model 02: Complex lm Model\")\n\n\n\n\n\n\n##----.Calculate the metrics using the helper function for Model 02----\nmetrics_model_02 <- model_02_linear_lm_complex %>% calc_metrics(test_tbl)\n\n#> Warning: There are new levels in a factor: E-Road\n\n##----.Print the calculated metrics for Model 02----\nprint(metrics_model_02)\n\n#> # A tibble: 3 × 3\n#>   .metric .estimator .estimate\n#>   <chr>   <chr>          <dbl>\n#> 1 rmse    standard    1053.   \n#> 2 rsq     standard       0.501\n#> 3 mae     standard     768.\n\n##----.TESTING THE ALGORITHMS OUT----\ng1 <- bike_features_tbl %>% \n  mutate(category_2 = as.factor(category_2) %>% \n           fct_reorder(price)) %>% \n  \n  ggplot(aes(category_2, price)) +\n  geom_violin() +\n  geom_jitter(width = 0.1, alpha = 0.5, color = \"#2dc6d6\") +\n  coord_flip() +\n  facet_wrap(~ frame_material) +\n  scale_y_continuous(labels = scales::dollar_format()) +\n  labs(\n    title = \"Unit Price for Each Model\",\n    y = \"\", x = \"Category 2\"\n  )\n\n##----.Print the plot----\nprint(g1)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n\n\n\n\n\n\n\n##----.NEW MODEL----\n\nnew_cross_country <- tibble(\n  model = \"Exceed AL SL new\",\n  category_2 = \"Cross-Country\",\n  frame_material = \"aluminium\",\n  shimano_dura_ace = 0,\n  shimano_ultegra = 0,\n  shimano_105 = 0,\n  shimano_tiagra = 0,\n  Shimano_sora = 0,\n  shimano_deore = 0,\n  shimano_slx = 0,\n  shimano_grx = 0,\n  Shimano_xt = 1,\n  Shimano_xtr = 0,\n  Shimano_saint = 0,\n  SRAM_red = 0,\n  SRAM_force = 0,\n  SRAM_rival = 0,\n  SRAM_apex = 0,\n  SRAM_xx1 = 0,\n  SRAM_x01 = 0,\n  SRAM_gx = 0,\n  SRAM_nx = 0,\n  SRAM_sx = 0,\n  Campagnolo_potenza = 0,\n  Campagnolo_super_record = 0,\n  shimano_nexus = 0,\n  shimano_alfine = 0\n) \n\nnew_cross_country\n\n\n\n  \n\n\n##----.Linear Methods----\n\n# Iteration\nmodels_tbl <- tibble(\n  model_id = str_c(\"Model 0\", 1:2),\n  model = list(\n    model_01_linear_lm_simple,\n    model_02_linear_lm_complex\n  )\n)\n\nmodels_tbl\n\n\n\n  \n\n\n##----.Add Predictions----\npredictions_new_cross_country_tbl <- models_tbl %>%\n  mutate(predictions = map(model, predict, new_data = new_cross_country)) %>%\n  unnest(predictions) %>%\n  mutate(category_2 = \"Cross-Country\") %>%\n  left_join(new_cross_country, by = \"category_2\")\n\npredictions_new_cross_country_tbl\n\n\n\n  \n\n\n##----.Update plot----\ng2 <- g1 +\n  geom_point(aes(y = .pred), color = \"red\", alpha = 0.5,\n             data = predictions_new_cross_country_tbl) +\n  ggrepel::geom_text_repel(aes(label = model_id, y = .pred),\n                           size = 3,\n                           data = predictions_new_cross_country_tbl)\nprint(g2)\n\n#> Warning: Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped.\n#> Groups with fewer than two data points have been dropped."
  },
  {
    "objectID": "content/01_journal/03_Automated_Machine_Learning_with_H20_(I).html",
    "href": "content/01_journal/03_Automated_Machine_Learning_with_H20_(I).html",
    "title": "Automated Machine Learning with H20 (I)",
    "section": "",
    "text": "1 Compensation Features:\nWhat can you deduce about the interaction between Monthly Income and Attrition?\na)Those that are leaving the company have a higher Monthly Income\nb)That those are staying have a lower Monthly Income\nc)Those that are leaving have a lower Monthly Income\nd)It’s difficult to deduce anything based on the visualization\n\n2 Compensation Features:\nWhat can you deduce about the interaction between Percent Salary Hike and Attrition?\na)Those that are leaving the company have a higher Percent Salary Hike\nb)Those that are staying have a lower Percent Salary Hike\nc)Those that are leaving have lower Percent Salary Hike\nd)It’s difficult to deduce anything based on the visualization\n\n3 Compensation Features:\nWhat can you deduce about the interaction between Stock Option Level and Attrition?\na)Those that are leaving the company have a higher stock option level\nb)Those that are staying have a higher stock option level\nc)It’s difficult to deduce anything based on the visualization\n\n4 Survey Results\nWhat can you deduce about the interaction between Environment Satisfaction and Attrition?\na)A higher proportion of those leaving have a low environment satisfaction level\nb)A higher proportion of those leaving have a high environment satisfaction level\nc)It’s difficult to deduce anything based on the visualization\n\n5 Survey Results\nWhat can you deduce about the interaction between Work Life Balance and Attrition?\na)Those that are leaving have higher density of 2’s and 3’s\nb)Those that are staying have a higher density of 2’s and 3’s\nc)Those that are staying have a lower density of 2’s and 3’s\nd)It’s difficult to deduce anything based on the visualization\n\n6 Performance Data\nWhat Can you deduce about the interaction between Job Involvement and Attrition?\na)Those that are leaving have a lower density of 3’s and 4’s\nb)Those that are leaving have a lower density of 1’s and 2’s\nc)Those that are staying have a lower density of 2’s and 3’s\nd)It’s difficult to deduce anything based on the visualization\n\n7 Work-Life Features\nWhat can you deduce about the interaction between Over Time and Attrition?\na)The proportion of those leaving that are working Over Time are high compared to those that are not leaving\nb)The proportion of those staying that are working Over Time are high compared to those that are not staying\n\n8 Training and Education\nWhat can you deduce about the interaction between Training Times Last Year and Attrition\na)People that leave tend to have more annual trainings\nb)People that leave tend to have less annual trainings\nc)It’s difficult to deduce anything based on the visualization\n\n9 Time-Based Features\nWhat can you deduce about the interaction between Years At Company and Attrition\na)People that leave tend to have more working years at the company\nb)People that leave tend to have less working years at the company\nc)It’s difficult to deduce anything based on the visualization\n\n10 Time-Based Features\nWhat can you deduce about the interaction between Years Since Last Promotion and Attrition?\na)Those that are leaving have more years since last promotion than those that are staying\nb)Those that are leaving have fewer years since last promotion than those that are staying\nc)It’s difficult to deduce anything based on the visualization\n\n11 code\n\n##----.libraries----\nlibrary(tidyverse)\n\n#> ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n#> ✔ dplyr     1.1.2     ✔ readr     2.1.4\n#> ✔ forcats   1.0.0     ✔ stringr   1.5.0\n#> ✔ ggplot2   3.4.2     ✔ tibble    3.2.1\n#> ✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n#> ✔ purrr     1.0.1     \n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> ✖ dplyr::filter() masks stats::filter()\n#> ✖ dplyr::lag()    masks stats::lag()\n#> ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors\n\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyverse)\nlibrary(readxl)\nlibrary(skimr)\nlibrary(GGally)\n\n#> Registered S3 method overwritten by 'GGally':\n#>   method from   \n#>   +.gg   ggplot2\n\n##----.Load data-----\nemployee_attrition_tbl <- read_csv(\"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/datasets-1067-1925-WA_Fn-UseC_-HR-Employee-Attrition.csv\")\n\n#> Rows: 1470 Columns: 35\n#> ── Column specification ────────────────────────────────────────────────────────\n#> Delimiter: \",\"\n#> chr  (9): Attrition, BusinessTravel, Department, EducationField, Gender, Job...\n#> dbl (26): Age, DailyRate, DistanceFromHome, Education, EmployeeCount, Employ...\n#> \n#> ℹ Use `spec()` to retrieve the full column specification for this data.\n#> ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\n\ndept_job_role_tbl <- employee_attrition_tbl %>%\nselect(EmployeeNumber, Department, JobRole, PerformanceRating, Attrition)\n\n##----.Function to convert counts to percentages---- \ncount_to_pct <- function(data, ..., col = n) {\n  \n  # capture the dots\n  grouping_vars_expr <- quos(...)\n  col_expr <- enquo(col)\n  \n  ret <- data %>%\n    group_by(!!! grouping_vars_expr) %>%\n    mutate(pct = (!! col_expr) / sum(!! col_expr)) %>%\n    ungroup()\n  \n  return(ret)\n  \n}\n\n\n##----.Develop KPI function----\n\nassess_attrition <- function(data, attrition_col, attrition_value, baseline_pct) {\n  \n  attrition_col_expr <- enquo(attrition_col)\n  \n  data %>%\n    \n    # Use parenthesis () to give tidy eval evaluation priority\n    filter((!! attrition_col_expr) %in% attrition_value) %>%\n    arrange(desc(pct)) %>%\n    mutate(\n      # Function inputs in numeric format\n      above_industry_avg = case_when(\n        pct > baseline_pct ~ \"Yes\",\n        TRUE ~ \"No\"\n      )\n    )\n  \n}\n\n\n##----.Function to calculate attrition cost----\ncalculate_attrition_cost <- function(\n    \n  # Employee\n  n                    = 1,\n  salary               = 80000,\n  \n  # Direct Costs\n  separation_cost      = 500,\n  vacancy_cost         = 10000,\n  acquisition_cost     = 4900,\n  placement_cost       = 3500,\n  \n  # Productivity Costs\n  net_revenue_per_employee = 250000,\n  workdays_per_year        = 240,\n  workdays_position_open   = 40,\n  workdays_onboarding      = 60,\n  onboarding_efficiency    = 0.50\n  \n) {\n  \n  # Direct Costs\n  direct_cost <- sum(separation_cost, vacancy_cost, acquisition_cost, placement_cost)\n  \n  # Lost Productivity Costs\n  productivity_cost <- net_revenue_per_employee / workdays_per_year *\n    (workdays_position_open + workdays_onboarding * onboarding_efficiency)\n  \n  # Savings of Salary & Benefits (Cost Reduction)\n  salary_benefit_reduction <- salary / workdays_per_year * workdays_position_open\n  \n  # Estimated Turnover Per Employee\n  cost_per_employee <- direct_cost + productivity_cost - salary_benefit_reduction\n  \n  # Total Cost of Employee Turnover\n  total_cost <- n * cost_per_employee\n  \n  return(total_cost)\n  \n}\n\n\n##----.Function to plot attrition----\nplot_attrition <- function(data, \n                           ..., \n                           .value,\n                           fct_reorder = TRUE,\n                           fct_rev     = FALSE,\n                           include_lbl = TRUE,\n                           color       = \"#2dc6d6\",\n                           units       = c(\"0\", \"K\", \"M\")) {\n  \n  ### Inputs\n  group_vars_expr   <- quos(...)\n  \n  # If the user does not supply anything, \n  # this takes the first column of the supplied data\n  if (length(group_vars_expr) == 0) {\n    group_vars_expr <- quos(rlang::sym(colnames(data)[[1]]))\n  }\n  \n  value_expr <- enquo(.value)\n  \n  units_val  <- switch(units[[1]],\n                       \"M\" = 1e6,\n                       \"K\" = 1e3,\n                       \"0\" = 1)\n  if (units[[1]] == \"0\") units <- \"\"\n  \n  # Data Manipulation\n  # This is a so called Function Factory (a function that produces a function)\n  usd <- scales::dollar_format(prefix = \"$\", largest_with_cents = 1e3)\n  \n  # Create the axis labels and values for the plot\n  data_manipulated <- data %>%\n    mutate(name = str_c(!!! group_vars_expr, sep = \": \") %>% as_factor()) %>%\n    mutate(value_text = str_c(usd(!! value_expr / units_val),\n                              units[[1]], sep = \"\"))\n  \n  \n  # Order the labels on the y-axis according to the input\n  if (fct_reorder) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_reorder(name, !! value_expr)) %>%\n      arrange(name)\n  }\n  \n  if (fct_rev) {\n    data_manipulated <- data_manipulated %>%\n      mutate(name = forcats::fct_rev(name)) %>%\n      arrange(name)\n  }\n  \n\n##----.Visualization----\n  \n  g <- data_manipulated %>%\n    \n    # \"name\" is a column name generated by our function internally as part of the data manipulation task\n    ggplot(aes(x = (!! value_expr), y = name)) +\n    geom_segment(aes(xend = 0, yend = name), color = color) +\n    geom_point(aes(size = !! value_expr), color = color) +\n    scale_x_continuous(labels = scales::dollar) +\n    scale_size(range = c(3, 5)) +\n    theme(legend.position = \"none\")\n  \n  # Plot labels if TRUE\n  if (include_lbl) {\n    g <- g +\n      geom_label(aes(label = value_text, size = !! value_expr),\n                 hjust = \"inward\", color = color)\n  }\n  \n  return(g)\n}\n\n##----.calculations----\nattrition_cost_df <- dept_job_role_tbl %>%\n  count(Department, JobRole, Attrition) %>%\n  count_to_pct(Department, JobRole)  %>%  \n  assess_attrition(Attrition, attrition_value = \"Yes\", baseline_pct = 0.088) %>%\n  mutate(\n    cost_of_attrition = calculate_attrition_cost(n = n, salary = 80000)\n  ) %>%\n  # Select columnns\n  plot_attrition(Department, JobRole, .value = cost_of_attrition,\n                 units = \"M\") +\n  labs(\n    title = \"Estimated Cost of Attrition by Job Role\",\n    x = \"Cost of Attrition\",\n    subtitle = \"Looks like Sales Executive and Labaratory Technician are the biggest drivers of cost\"\n  )\n  \n\n\n##----.Custom plotting function----\n\ndata <- employee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome)\n\nplot_ggpairs <- function(data, color = NULL, density_alpha = 0.5) {\n  \n  color_expr <- enquo(color)\n  \n  if (rlang::quo_is_null(color_expr)) {\n    \n    g <- data %>%\n      ggpairs(lower = \"blank\") \n    \n  } else {\n    \n    color_name <- quo_name(color_expr)\n    \n    g <- data %>%\n      ggpairs(mapping = aes_string(color = color_name), \n              lower = \"blank\", legend = 1,\n              diag = list(continuous = wrap(\"densityDiag\", \n                                            alpha = density_alpha))) +\n      theme(legend.position = \"bottom\")\n  }\n  \n  return(g)\n  \n}\n\nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(color = Attrition)\n\n#> Warning: `aes_string()` was deprecated in ggplot2 3.0.0.\n#> ℹ Please use tidy evaluation idioms with `aes()`.\n#> ℹ See also `vignette(\"ggplot2-in-packages\")` for more information.\n\n\n\n\n\n\n\n##----.Explore Features by Category----\n\n#   1. Descriptive features: age, gender, marital status \nemployee_attrition_tbl %>%\n  select(Attrition, Age, Gender, MaritalStatus, NumCompaniesWorked, Over18, DistanceFromHome) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   2. Employment features: department, job role, job level\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"employee\"), contains(\"department\"), contains(\"job\")) %>%\n  plot_ggpairs(Attrition) \n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n#> Warning in cor(x, y): the standard deviation is zero\n\n\n\n\n\n\n\n#   3. Compensation features: HourlyRate, MonthlyIncome, StockOptionLevel \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"income\"), contains(\"rate\"), contains(\"salary\"), contains(\"stock\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   4. Survey Results: Satisfaction level, WorkLifeBalance \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"satisfaction\"), contains(\"life\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   5. Performance Data: Job Involvement, Performance Rating\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"performance\"), contains(\"involvement\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   6. Work-Life Features \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"overtime\"), contains(\"travel\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   7. Training and Education \nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"training\"), contains(\"education\")) %>%\n  plot_ggpairs(Attrition)\n\n\n\n\n\n\n#   8. Time-Based Features: Years at company, years in current role\nemployee_attrition_tbl %>%\n  select(Attrition, contains(\"years\")) %>%\n  plot_ggpairs(Attrition)"
  },
  {
    "objectID": "content/01_journal/04_Automated_Machine_Learning_with_H20_(II).html",
    "href": "content/01_journal/04_Automated_Machine_Learning_with_H20_(II).html",
    "title": "Automated Machine Learning with H20 (II)",
    "section": "",
    "text": "1 challenge\n\n##----.libraries----\nlibrary(h2o)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(purrr)\n\n##----.Initialize H2O----\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         5 minutes 1 seconds \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_mosta_svg245 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   2.98 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\n##----.Load the training, validation, and test datasets----\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.response and predictor variables----\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n##----.AutoML----\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 12:22:18.911: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 12:22:18.911: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  38%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |================================                                      |  45%\n  |                                                                            \n  |==================================                                    |  49%\n  |                                                                            \n  |=====================================                                 |  53%\n  |                                                                            \n  |=======================================                               |  56%\n  |                                                                            \n  |==========================================                            |  60%\n  |                                                                            \n  |============================================                          |  64%\n  |                                                                            \n  |===============================================                       |  67%\n  |                                                                            \n  |=================================================                     |  71%\n  |                                                                            \n  |====================================================                  |  74%\n  |                                                                            \n  |======================================================                |  78%\n  |                                                                            \n  |=========================================================             |  81%\n  |                                                                            \n  |============================================================          |  85%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.leaderboard----\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                  model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_3_20230607_122218 0.9554813 0.1646200\n#> 2    StackedEnsemble_AllModels_1_AutoML_3_20230607_122218 0.9551619 0.1650368\n#> 3 StackedEnsemble_BestOfFamily_3_AutoML_3_20230607_122218 0.9545483 0.1664879\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_3_20230607_122218 0.9538884 0.1677040\n#> 5                          GBM_4_AutoML_3_20230607_122218 0.9531997 0.1689092\n#> 6                          GBM_3_AutoML_3_20230607_122218 0.9528312 0.1706504\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7602304            0.1333442 0.2216200 0.04911544\n#> 2 0.7585778            0.1275209 0.2219367 0.04925589\n#> 3 0.7564890            0.1367932 0.2223929 0.04945860\n#> 4 0.7523904            0.1443508 0.2232434 0.04983762\n#> 5 0.7517860            0.1357064 0.2242145 0.05027215\n#> 6 0.7442681            0.1396240 0.2255727 0.05088305\n#> \n#> [17 rows x 7 columns]\n\n##----leader model----\nleader_model <- automl_models_h2o@leader\nleader_model\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_2_AutoML_3_20230607_122218 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)              4/9\n#> 3           # GBM base models (used / total)              3/5\n#> 4           # DRF base models (used / total)              1/2\n#> 5           # GLM base models (used / total)              0/1\n#> 6  # DeepLearning base models (used / total)              0/1\n#> 7                      Metalearner algorithm              GLM\n#> 8         Metalearner fold assignment scheme           Random\n#> 9                         Metalearner nfolds                5\n#> 10                   Metalearner fold_column               NA\n#> 11        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02576233\n#> RMSE:  0.1605065\n#> LogLoss:  0.09475967\n#> Mean Per-Class Error:  0.08100955\n#> AUC:  0.9899324\n#> AUCPR:  0.9388804\n#> Gini:  0.9798649\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8753  160 0.017951   =160/8913\n#> Yes     170 1010 0.144068   =170/1180\n#> Totals 8923 1170 0.032696  =330/10093\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.428451    0.859574 173\n#> 2                       max f2  0.261137    0.890610 225\n#> 3                 max f0point5  0.593135    0.886546 131\n#> 4                 max accuracy  0.486267    0.967601 158\n#> 5                max precision  0.988006    1.000000   0\n#> 6                   max recall  0.017611    1.000000 361\n#> 7              max specificity  0.988006    1.000000   0\n#> 8             max absolute_mcc  0.428451    0.841083 173\n#> 9   max min_per_class_accuracy  0.217375    0.948053 241\n#> 10 max mean_per_class_accuracy  0.173691    0.950907 260\n#> 11                     max tns  0.988006 8913.000000   0\n#> 12                     max fns  0.988006 1179.000000   0\n#> 13                     max fps  0.000218 8913.000000 399\n#> 14                     max tps  0.017611 1180.000000 361\n#> 15                     max tnr  0.988006    1.000000   0\n#> 16                     max fnr  0.988006    0.999153   0\n#> 17                     max fpr  0.000218    1.000000 399\n#> 18                     max tpr  0.017611    1.000000 361\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.02616927\n#> RMSE:  0.1617692\n#> LogLoss:  0.09670113\n#> Mean Per-Class Error:  0.08181059\n#> AUC:  0.9894601\n#> AUCPR:  0.940179\n#> Gini:  0.9789201\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error        Rate\n#> No     16485  302 0.017990  =302/16787\n#> Yes      330 1936 0.145631   =330/2266\n#> Totals 16815 2238 0.033171  =632/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.428771     0.859680 179\n#> 2                       max f2  0.265840     0.887850 231\n#> 3                 max f0point5  0.602916     0.892516 130\n#> 4                 max accuracy  0.484175     0.967039 163\n#> 5                max precision  0.988388     1.000000   0\n#> 6                   max recall  0.018195     1.000000 366\n#> 7              max specificity  0.988388     1.000000   0\n#> 8             max absolute_mcc  0.428771     0.840894 179\n#> 9   max min_per_class_accuracy  0.217630     0.945434 249\n#> 10 max mean_per_class_accuracy  0.199183     0.948316 256\n#> 11                     max tns  0.988388 16787.000000   0\n#> 12                     max fns  0.988388  2259.000000   0\n#> 13                     max fps  0.000150 16787.000000 399\n#> 14                     max tps  0.018195  2266.000000 366\n#> 15                     max tnr  0.988388     1.000000   0\n#> 16                     max fnr  0.988388     0.996911   0\n#> 17                     max fpr  0.000150     1.000000 399\n#> 18                     max tpr  0.018195     1.000000 366\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04911544\n#> RMSE:  0.22162\n#> LogLoss:  0.16462\n#> Mean Per-Class Error:  0.1333442\n#> AUC:  0.9554813\n#> AUCPR:  0.7602304\n#> Gini:  0.9109626\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     15829  958 0.057068   =958/16787\n#> Yes      475 1791 0.209620    =475/2266\n#> Totals 16304 2749 0.075211  =1433/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.280932     0.714257 227\n#> 2                       max f2  0.153478     0.786508 276\n#> 3                 max f0point5  0.590009     0.738244 132\n#> 4                 max accuracy  0.530186     0.932399 150\n#> 5                max precision  0.992470     1.000000   0\n#> 6                   max recall  0.000389     1.000000 399\n#> 7              max specificity  0.992470     1.000000   0\n#> 8             max absolute_mcc  0.280932     0.675570 227\n#> 9   max min_per_class_accuracy  0.123123     0.892321 289\n#> 10 max mean_per_class_accuracy  0.093141     0.894898 306\n#> 11                     max tns  0.992470 16787.000000   0\n#> 12                     max fns  0.992470  2264.000000   0\n#> 13                     max fps  0.000389 16787.000000 399\n#> 14                     max tps  0.000389  2266.000000 399\n#> 15                     max tnr  0.992470     1.000000   0\n#> 16                     max fnr  0.992470     0.999117   0\n#> 17                     max fpr  0.000389     1.000000 399\n#> 18                     max tpr  0.000389     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.928883  0.006950   0.933264   0.928211   0.918626   0.937002\n#> auc         0.955693  0.003417   0.954147   0.957676   0.951477   0.960364\n#> err         0.071117  0.006950   0.066736   0.071789   0.081374   0.062998\n#> err_count 270.800000 24.118458 255.000000 270.000000 308.000000 245.000000\n#> f0point5    0.695162  0.022954   0.698467   0.692220   0.670122   0.731547\n#>           cv_5_valid\n#> accuracy    0.927311\n#> auc         0.954799\n#> err         0.072689\n#> err_count 276.000000\n#> f0point5    0.683453\n#> \n#> ---\n#>                          mean        sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.679658  0.032212    0.684760    0.669742    0.646552\n#> r2                   0.531421  0.018991    0.522256    0.539250    0.506773\n#> recall               0.768083  0.026319    0.759259    0.799559    0.784519\n#> residual_deviance 1254.122200 58.529808 1243.656400 1226.594500 1354.737800\n#> rmse                 0.221547  0.007137    0.218876    0.221144    0.233286\n#> specificity          0.950492  0.010002    0.955444    0.945872    0.938010\n#>                    cv_4_valid  cv_5_valid\n#> precision            0.731868    0.665370\n#> r2                   0.557685    0.531143\n#> recall               0.730263    0.766816\n#> residual_deviance 1203.428600 1242.193500\n#> rmse                 0.213967    0.220462\n#> specificity          0.964463    0.948672\n\n##----.Predict using the leader model----\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n##----.Set the directory path to save the leader model----\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n##----.Save the leader model----\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_3_20230607_122218\""
  },
  {
    "objectID": "content/01_journal/05_Performance_Measures.html",
    "href": "content/01_journal/05_Performance_Measures.html",
    "title": "Performance Measures",
    "section": "",
    "text": "1 Leaderboard Visualization and Tune a Model with Grid Search\n\n##----.libraries----\nlibrary(h2o)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(purrr)\n\n##----.Initialize H2O----\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         1 hours 13 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_mosta_iae628 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   2.99 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nh2o.removeAll()\n##----.Load the training, validation, and test datasets----\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.the response and predictor variables----\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n##----.AutoML----\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 14:44:28.49: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 14:44:28.49: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  13%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |==============                                                        |  20%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |===================                                                   |  27%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |========================                                              |  34%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |=============================                                         |  41%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |==================================                                    |  48%\n  |                                                                            \n  |====================================                                  |  52%\n  |                                                                            \n  |=======================================                               |  55%\n  |                                                                            \n  |=========================================                             |  59%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  66%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |===================================================                   |  73%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |========================================================              |  80%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |================================================================      |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |===================================================================== |  99%\n  |                                                                            \n  |======================================================================| 100%\n\n##----leaderboard----\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                   model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_10_20230607_144428 0.9557575 0.1643098\n#> 2    StackedEnsemble_AllModels_1_AutoML_10_20230607_144428 0.9552891 0.1649082\n#> 3 StackedEnsemble_BestOfFamily_3_AutoML_10_20230607_144428 0.9550695 0.1664009\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_10_20230607_144428 0.9538952 0.1681380\n#> 5                          GBM_4_AutoML_10_20230607_144428 0.9527815 0.1702775\n#> 6                          GBM_3_AutoML_10_20230607_144428 0.9521364 0.1713910\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7627748            0.1431758 0.2217429 0.04916989\n#> 2 0.7611167            0.1424773 0.2222570 0.04939818\n#> 3 0.7564759            0.1417260 0.2229967 0.04972751\n#> 4 0.7517814            0.1369829 0.2241009 0.05022122\n#> 5 0.7492407            0.1446121 0.2255109 0.05085516\n#> 6 0.7467296            0.1417600 0.2262253 0.05117788\n#> \n#> [15 rows x 7 columns]\n\n##----Extract the leader model----\nleader_model <- automl_models_h2o@leader\nleader_model\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_2_AutoML_10_20230607_144428 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)              6/9\n#> 3           # GBM base models (used / total)              4/5\n#> 4           # DRF base models (used / total)              2/2\n#> 5           # GLM base models (used / total)              0/1\n#> 6  # DeepLearning base models (used / total)              0/1\n#> 7                      Metalearner algorithm              GLM\n#> 8         Metalearner fold assignment scheme           Random\n#> 9                         Metalearner nfolds                5\n#> 10                   Metalearner fold_column               NA\n#> 11        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02654229\n#> RMSE:  0.162918\n#> LogLoss:  0.09804217\n#> Mean Per-Class Error:  0.07622281\n#> AUC:  0.9883027\n#> AUCPR:  0.9385305\n#> Gini:  0.9766055\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8662  185 0.020911   =185/8847\n#> Yes     156 1030 0.131535   =156/1186\n#> Totals 8818 1215 0.033988  =341/10033\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.408844    0.857976 181\n#> 2                       max f2  0.271171    0.886137 224\n#> 3                 max f0point5  0.593553    0.893939 131\n#> 4                 max accuracy  0.455478    0.966710 168\n#> 5                max precision  0.995628    1.000000   0\n#> 6                   max recall  0.018283    1.000000 361\n#> 7              max specificity  0.995628    1.000000   0\n#> 8             max absolute_mcc  0.408844    0.838754 181\n#> 9   max min_per_class_accuracy  0.222344    0.942664 240\n#> 10 max mean_per_class_accuracy  0.271171    0.943563 224\n#> 11                     max tns  0.995628 8847.000000   0\n#> 12                     max fns  0.995628 1185.000000   0\n#> 13                     max fps  0.000242 8847.000000 399\n#> 14                     max tps  0.018283 1186.000000 361\n#> 15                     max tnr  0.995628    1.000000   0\n#> 16                     max fnr  0.995628    0.999157   0\n#> 17                     max fpr  0.000242    1.000000 399\n#> 18                     max tpr  0.018283    1.000000 361\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.0270699\n#> RMSE:  0.1645293\n#> LogLoss:  0.09877566\n#> Mean Per-Class Error:  0.08583523\n#> AUC:  0.9884169\n#> AUCPR:  0.9353316\n#> Gini:  0.9768339\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error        Rate\n#> No     16461  326 0.019420  =326/16787\n#> Yes      345 1921 0.152251   =345/2266\n#> Totals 16806 2247 0.035218  =671/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.433480     0.851318 177\n#> 2                       max f2  0.273263     0.882824 229\n#> 3                 max f0point5  0.628936     0.890717 121\n#> 4                 max accuracy  0.505103     0.965202 157\n#> 5                max precision  0.997616     1.000000   0\n#> 6                   max recall  0.018668     1.000000 365\n#> 7              max specificity  0.997616     1.000000   0\n#> 8             max absolute_mcc  0.458143     0.831416 171\n#> 9   max min_per_class_accuracy  0.220841     0.943071 248\n#> 10 max mean_per_class_accuracy  0.212806     0.943837 251\n#> 11                     max tns  0.997616 16787.000000   0\n#> 12                     max fns  0.997616  2264.000000   0\n#> 13                     max fps  0.000169 16787.000000 399\n#> 14                     max tps  0.018668  2266.000000 365\n#> 15                     max tnr  0.997616     1.000000   0\n#> 16                     max fnr  0.997616     0.999117   0\n#> 17                     max fpr  0.000169     1.000000 399\n#> 18                     max tpr  0.018668     1.000000 365\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04916989\n#> RMSE:  0.2217429\n#> LogLoss:  0.1643098\n#> Mean Per-Class Error:  0.1431758\n#> AUC:  0.9557575\n#> AUCPR:  0.7627748\n#> Gini:  0.9115151\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     15936  851 0.050694   =851/16787\n#> Yes      534 1732 0.235658    =534/2266\n#> Totals 16470 2583 0.072692  =1385/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.319510     0.714374 209\n#> 2                       max f2  0.121180     0.784412 287\n#> 3                 max f0point5  0.607344     0.737377 123\n#> 4                 max accuracy  0.502595     0.932557 153\n#> 5                max precision  0.990475     1.000000   0\n#> 6                   max recall  0.000176     1.000000 399\n#> 7              max specificity  0.990475     1.000000   0\n#> 8             max absolute_mcc  0.288570     0.675364 220\n#> 9   max min_per_class_accuracy  0.123877     0.891166 286\n#> 10 max mean_per_class_accuracy  0.090379     0.894489 303\n#> 11                     max tns  0.990475 16787.000000   0\n#> 12                     max fns  0.990475  2263.000000   0\n#> 13                     max fps  0.000176 16787.000000 399\n#> 14                     max tps  0.000176  2266.000000 399\n#> 15                     max tnr  0.990475     1.000000   0\n#> 16                     max fnr  0.990475     0.998676   0\n#> 17                     max fpr  0.000176     1.000000 399\n#> 18                     max tpr  0.000176     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.928605  0.007432   0.939979   0.928478   0.927679   0.919118\n#> auc         0.956060  0.007263   0.963522   0.959256   0.944056   0.956491\n#> err         0.071395  0.007432   0.060021   0.071522   0.072320   0.080882\n#> err_count 272.000000 27.856777 230.000000 274.000000 278.000000 308.000000\n#> f0point5    0.694470  0.027955   0.717381   0.681188   0.708316   0.651450\n#>           cv_5_valid\n#> accuracy    0.927769\n#> auc         0.956974\n#> err         0.072231\n#> err_count 270.000000\n#> f0point5    0.714017\n#> \n#> ---\n#>                          mean         sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.679140   0.042145    0.714628    0.660269    0.707724\n#> r2                   0.530941   0.017559    0.547628    0.534900    0.505012\n#> recall               0.772463   0.059313    0.728606    0.780045    0.710692\n#> residual_deviance 1250.589800 109.281220 1105.648400 1205.126000 1405.014600\n#> rmse                 0.221588   0.009344    0.207677    0.217661    0.231950\n#> specificity          0.949599   0.014587    0.965235    0.947788    0.958420\n#>                    cv_4_valid  cv_5_valid\n#> precision            0.613815    0.699262\n#> r2                   0.522336    0.544826\n#> recall               0.863135    0.779835\n#> residual_deviance 1258.573500 1278.586700\n#> rmse                 0.223748    0.226904\n#> specificity          0.926677    0.949877\n\n##----.Predict using the leader model----\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n##----.Set the directory path to save the leader model-----\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n##----.Save the leader model----\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_10_20230607_144428\"\n\n##----.Visualize the leaderboard----\nplot_h2o_leaderboard <- function(h2o_leaderboard, order_by = c(\"auc\", \"logloss\"),\n                                 n_max = 20, size = 4, include_lbl = TRUE) {\n  # Setup inputs\n  order_by <- tolower(order_by[[1]])\n  \n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    select(-c(aucpr, mean_per_class_error, rmse, mse)) %>% \n    mutate(model_type = str_extract(model_id, \"[^_]+\")) %>%\n    rownames_to_column(var = \"rowname\") %>%\n    mutate(model_id = paste0(rowname, \". \", model_id) %>% as.factor())\n  \n  # Transformation\n  if (order_by == \"auc\") {\n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id = as_factor(model_id) %>% reorder(auc),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n  } else if (order_by == \"logloss\") {\n    data_transformed_tbl <- leaderboard_tbl %>%\n      slice(1:n_max) %>%\n      mutate(\n        model_id = as_factor(model_id) %>% reorder(logloss) %>% fct_rev(),\n        model_type = as.factor(model_type)\n      ) %>%\n      pivot_longer(cols = -c(model_id, model_type, rowname), \n                   names_to = \"key\", \n                   values_to = \"value\", \n                   names_transform = list(key = forcats::fct_inorder)\n      )\n  } else {\n    # If nothing is supplied\n    stop(paste0(\"order_by = '\", order_by, \"' is not a permitted option.\"))\n  }\n  \n  # Visualization\n  g <- data_transformed_tbl %>%\n    ggplot(aes(value, model_id, color = model_type)) +\n    geom_point(size = size) +\n    facet_wrap(~ key, scales = \"free_x\") +\n    labs(title = \"Leaderboard Metrics\",\n         subtitle = paste0(\"Ordered by: \", toupper(order_by)),\n         y = \"Model Position, Model ID\", x = \"\")\n  \n  if (include_lbl) g <- g + geom_label(aes(label = round(value, 2), hjust = \"inward\"))\n  \n  return(g)\n}\n\nplot_h2o_leaderboard(automl_models_h2o@leaderboard, order_by = \"auc\", n_max = 15)\n\n\n\n\n\n\n##----.grid search----\n\n# Deeplearning algorithm\ndeeplearning_grid_01 <- h2o.grid(\n  algorithm = \"deeplearning\",\n  grid_id = \"deeplearning_grid\",\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  nfolds = 5,\n  hyper_params = list(\n    hidden = list(c(10, 10, 10), c(50, 20, 10), c(20, 20, 20)),\n    epochs = c(10, 50, 100)\n  )\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ndeeplearning_grid_01\n\n#> H2O Grid Details\n#> ================\n#> \n#> Grid ID: deeplearning_grid \n#> Used hyper parameters: \n#>   -  epochs \n#>   -  hidden \n#> Number of models: 9 \n#> Number of failed models: 0 \n#> \n#> Hyper-Parameter Search Summary: ordered by increasing logloss\n#>      epochs       hidden                 model_ids logloss\n#> 1  10.40046 [10, 10, 10] deeplearning_grid_model_1 0.72742\n#> 2  10.39149 [50, 20, 10] deeplearning_grid_model_4 1.01268\n#> 3  51.99694 [10, 10, 10] deeplearning_grid_model_2 1.03460\n#> 4 104.93231 [10, 10, 10] deeplearning_grid_model_3 1.68339\n#> 5  10.39222 [20, 20, 20] deeplearning_grid_model_7 1.73358\n#> 6  51.99129 [50, 20, 10] deeplearning_grid_model_5 1.91117\n#> 7 104.96941 [50, 20, 10] deeplearning_grid_model_6 1.93688\n#> 8  52.00922 [20, 20, 20] deeplearning_grid_model_8 2.19749\n#> 9 104.97479 [20, 20, 20] deeplearning_grid_model_9 3.03588\n\nh2o.getGrid(grid_id = \"deeplearning_grid\", sort_by = \"auc\", decreasing = TRUE)\n\n#> H2O Grid Details\n#> ================\n#> \n#> Grid ID: deeplearning_grid \n#> Used hyper parameters: \n#>   -  epochs \n#>   -  hidden \n#> Number of models: 9 \n#> Number of failed models: 0 \n#> \n#> Hyper-Parameter Search Summary: ordered by decreasing auc\n#>      epochs       hidden                 model_ids     auc\n#> 1  51.99694 [10, 10, 10] deeplearning_grid_model_2 0.62744\n#> 2  10.40046 [10, 10, 10] deeplearning_grid_model_1 0.61960\n#> 3  10.39222 [20, 20, 20] deeplearning_grid_model_7 0.57671\n#> 4  10.39149 [50, 20, 10] deeplearning_grid_model_4 0.57598\n#> 5  51.99129 [50, 20, 10] deeplearning_grid_model_5 0.57559\n#> 6 104.96941 [50, 20, 10] deeplearning_grid_model_6 0.57153\n#> 7  52.00922 [20, 20, 20] deeplearning_grid_model_8 0.56415\n#> 8 104.97479 [20, 20, 20] deeplearning_grid_model_9 0.54965\n#> 9 104.93231 [10, 10, 10] deeplearning_grid_model_3 0.54719\n\ndeeplearning_grid_model_4 <- h2o.getModel(\"deeplearning_grid_model_4\")\n\ndeeplearning_grid_model_4 %>% h2o.auc(train = T, valid = T, xval = T)\n\n#>     train     valid      xval \n#> 0.7376786 0.7430833 0.5759754\n\n# Run it on the test data\ndeeplearning_grid_model_4 %>%\n  h2o.performance(newdata = as.h2o(test_df))\n\n#> H2OBinomialMetrics: deeplearning\n#> \n#> MSE:  0.1099523\n#> RMSE:  0.3315906\n#> LogLoss:  0.6607801\n#> Mean Per-Class Error:  0.2910898\n#> AUC:  0.7430833\n#> AUCPR:  0.2987794\n#> Gini:  0.4861666\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No   Yes    Error         Rate\n#> No     8503  8284 0.493477  =8284/16787\n#> Yes     201  2065 0.088703    =201/2266\n#> Totals 8704 10349 0.445337  =8485/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.000306     0.327388 396\n#> 2                       max f2  0.000306     0.531860 396\n#> 3                 max f0point5  0.067025     0.342133 255\n#> 4                 max accuracy  0.274108     0.884375 142\n#> 5                max precision  0.996738     1.000000   0\n#> 6                   max recall  0.000016     1.000000 399\n#> 7              max specificity  0.996738     1.000000   0\n#> 8             max absolute_mcc  0.000306     0.271517 396\n#> 9   max min_per_class_accuracy  0.001135     0.653575 389\n#> 10 max mean_per_class_accuracy  0.000306     0.708910 396\n#> 11                     max tns  0.996738 16787.000000   0\n#> 12                     max fns  0.996738  2265.000000   0\n#> 13                     max fps  0.000016 16787.000000 399\n#> 14                     max tps  0.000016  2266.000000 399\n#> 15                     max tnr  0.996738     1.000000   0\n#> 16                     max fnr  0.996738     0.999559   0\n#> 17                     max fpr  0.000016     1.000000 399\n#> 18                     max tpr  0.000016     1.000000 399\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n\n\n\n2 Visualize the trade of between the precision and the recall\n\n##----.Assessing Performance----\n\n# Extract the model from the leaderboard\nmodel1 <- automl_models_h2o@leaderboard[1, \"model_id\"] %>%\n  h2o.getModel()\n\nmodel2 <- automl_models_h2o@leaderboard[5, \"model_id\"] %>%\n  h2o.getModel()\n\nmodel3 <- automl_models_h2o@leaderboard[6, \"model_id\"] %>%\n  h2o.getModel()\n\npath1 <- h2o.saveModel(model1, path = save_directory, force = TRUE)\npath2 <- h2o.saveModel(model2, path = save_directory, force = TRUE)\npath3 <- h2o.saveModel(model3, path = save_directory, force = TRUE)\n\n\nperformance_h2o <- h2o.performance(model1, newdata = as.h2o(test_df))\n\ntypeof(performance_h2o)\n\n#> [1] \"S4\"\n\nperformance_h2o %>% slotNames()\n\n#> [1] \"algorithm\" \"on_train\"  \"on_valid\"  \"on_xval\"   \"metrics\"\n\nperformance_h2o@metrics\n\n#> $model\n#> $model$`__meta`\n#> $model$`__meta`$schema_version\n#> [1] 3\n#> \n#> $model$`__meta`$schema_name\n#> [1] \"ModelKeyV3\"\n#> \n#> $model$`__meta`$schema_type\n#> [1] \"Key<Model>\"\n#> \n#> \n#> $model$name\n#> [1] \"StackedEnsemble_AllModels_2_AutoML_9_20230607_143029\"\n#> \n#> $model$type\n#> [1] \"Key<Model>\"\n#> \n#> $model$URL\n#> [1] \"/3/Models/StackedEnsemble_AllModels_2_AutoML_9_20230607_143029\"\n#> \n#> \n#> $model_checksum\n#> [1] \"-6603105751155097888\"\n#> \n#> $frame\n#> $frame$name\n#> [1] \"test_df_sid_85a3_160\"\n#> \n#> \n#> $frame_checksum\n#> [1] \"340006888695133344\"\n#> \n#> $description\n#> NULL\n#> \n#> $scoring_time\n#> [1] 1.686141e+12\n#> \n#> $predictions\n#> NULL\n#> \n#> $MSE\n#> [1] 0.02694723\n#> \n#> $RMSE\n#> [1] 0.1641561\n#> \n#> $nobs\n#> [1] 19053\n#> \n#> $custom_metric_name\n#> NULL\n#> \n#> $custom_metric_value\n#> [1] 0\n#> \n#> $r2\n#> [1] 0.7428374\n#> \n#> $logloss\n#> [1] 0.09840563\n#> \n#> $AUC\n#> [1] 0.9885832\n#> \n#> $pr_auc\n#> [1] 0.9352975\n#> \n#> $Gini\n#> [1] 0.9771664\n#> \n#> $mean_per_class_error\n#> [1] 0.09127558\n#> \n#> $domain\n#> [1] \"No\"  \"Yes\"\n#> \n#> $cm\n#> $cm$`__meta`\n#> $cm$`__meta`$schema_version\n#> [1] 3\n#> \n#> $cm$`__meta`$schema_name\n#> [1] \"ConfusionMatrixV3\"\n#> \n#> $cm$`__meta`$schema_type\n#> [1] \"ConfusionMatrix\"\n#> \n#> \n#> $cm$table\n#> Confusion Matrix: Row labels: Actual class; Column labels: Predicted class\n#>           No  Yes  Error           Rate\n#> No     16508  279 0.0166 = 279 / 16,787\n#> Yes      376 1890 0.1659 =  376 / 2,266\n#> Totals 16884 2169 0.0344 = 655 / 19,053\n#> \n#> \n#> $thresholds_and_metric_scores\n#> Metrics for Thresholds: Binomial metrics as a function of classification thresholds\n#>   threshold       f1       f2 f0point5 accuracy precision   recall specificity\n#> 1  0.991773 0.009662 0.006061 0.023810 0.881646  1.000000 0.004854    1.000000\n#> 2  0.986198 0.020961 0.013204 0.050804 0.882328  1.000000 0.010591    1.000000\n#> 3  0.983182 0.026992 0.017042 0.064854 0.882696  1.000000 0.013680    1.000000\n#> 4  0.981155 0.040640 0.025793 0.095762 0.883535  1.000000 0.020741    1.000000\n#> 5  0.978747 0.053265 0.033969 0.123309 0.884323  1.000000 0.027361    1.000000\n#>   absolute_mcc min_per_class_accuracy mean_per_class_accuracy   tns  fns fps\n#> 1     0.065418               0.004854                0.502427 16787 2255   0\n#> 2     0.096662               0.010591                0.505296 16787 2242   0\n#> 3     0.109878               0.013680                0.506840 16787 2235   0\n#> 4     0.135351               0.020741                0.510371 16787 2219   0\n#> 5     0.155517               0.027361                0.513680 16787 2204   0\n#>   tps      tnr      fnr      fpr      tpr idx\n#> 1  11 1.000000 0.995146 0.000000 0.004854   0\n#> 2  24 1.000000 0.989409 0.000000 0.010591   1\n#> 3  31 1.000000 0.986320 0.000000 0.013680   2\n#> 4  47 1.000000 0.979259 0.000000 0.020741   3\n#> 5  62 1.000000 0.972639 0.000000 0.027361   4\n#> \n#> ---\n#>     threshold       f1       f2 f0point5 accuracy precision   recall\n#> 395  0.002305 0.249917 0.454436 0.172351 0.286097  0.142803 1.000000\n#> 396  0.001884 0.241269 0.442890 0.165794 0.251981  0.137184 1.000000\n#> 397  0.001453 0.234212 0.433303 0.160477 0.222275  0.132639 1.000000\n#> 398  0.001105 0.225293 0.420970 0.153802 0.182071  0.126947 1.000000\n#> 399  0.000805 0.221419 0.415536 0.150918 0.163596  0.124492 1.000000\n#> 400  0.000486 0.212580 0.402959 0.144372 0.118931  0.118931 1.000000\n#>     specificity absolute_mcc min_per_class_accuracy mean_per_class_accuracy\n#> 395    0.189730     0.164603               0.189730                0.594865\n#> 396    0.151010     0.143931               0.151010                0.575505\n#> 397    0.117293     0.124730               0.117293                0.558647\n#> 398    0.071663     0.095380               0.071663                0.535831\n#> 399    0.050694     0.079442               0.050694                0.525347\n#> 400    0.000000     0.000000               0.000000                0.500000\n#>      tns fns   fps  tps      tnr      fnr      fpr      tpr idx\n#> 395 3185   0 13602 2266 0.189730 0.000000 0.810270 1.000000 394\n#> 396 2535   0 14252 2266 0.151010 0.000000 0.848990 1.000000 395\n#> 397 1969   0 14818 2266 0.117293 0.000000 0.882707 1.000000 396\n#> 398 1203   0 15584 2266 0.071663 0.000000 0.928337 1.000000 397\n#> 399  851   0 15936 2266 0.050694 0.000000 0.949306 1.000000 398\n#> 400    0   0 16787 2266 0.000000 0.000000 1.000000 1.000000 399\n#> \n#> $max_criteria_and_metric_scores\n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.453855     0.852311 176\n#> 2                       max f2  0.273110     0.885847 234\n#> 3                 max f0point5  0.609546     0.887755 130\n#> 4                 max accuracy  0.483104     0.966042 166\n#> 5                max precision  0.991773     1.000000   0\n#> 6                   max recall  0.019939     1.000000 361\n#> 7              max specificity  0.991773     1.000000   0\n#> 8             max absolute_mcc  0.479417     0.833504 167\n#> 9   max min_per_class_accuracy  0.212110     0.943826 256\n#> 10 max mean_per_class_accuracy  0.205551     0.945262 258\n#> 11                     max tns  0.991773 16787.000000   0\n#> 12                     max fns  0.991773  2255.000000   0\n#> 13                     max fps  0.000486 16787.000000 399\n#> 14                     max tps  0.019939  2266.000000 361\n#> 15                     max tnr  0.991773     1.000000   0\n#> 16                     max fnr  0.991773     0.995146   0\n#> 17                     max fpr  0.000486     1.000000 399\n#> 18                     max tpr  0.019939     1.000000 361\n#> \n#> $gains_lift_table\n#> Gains/Lift Table: Avg response rate: 11.89 %, avg score: 12.30 %\n#>    group cumulative_data_fraction lower_threshold     lift cumulative_lift\n#> 1      1               0.01002467        0.958395 8.408208        8.408208\n#> 2      2               0.02004934        0.932003 8.364186        8.386197\n#> 3      3               0.03002152        0.902949 8.408208        8.393509\n#> 4      4               0.04004619        0.872483 8.364186        8.386168\n#> 5      5               0.05001837        0.842913 8.275447        8.364094\n#> 6      6               0.10003674        0.551497 7.031838        7.697966\n#> 7      7               0.15000262        0.259751 3.206071        6.201716\n#> 8      8               0.20002099        0.113750 0.882288        4.871510\n#> 9      9               0.30000525        0.037995 0.211860        3.318565\n#> 10    10               0.39998950        0.018707 0.044138        2.500066\n#> 11    11               0.50002624        0.011224 0.000000        1.999895\n#> 12    12               0.60001050        0.007286 0.000000        1.666638\n#> 13    13               0.69999475        0.004463 0.000000        1.428582\n#> 14    14               0.79997901        0.002549 0.000000        1.250033\n#> 15    15               0.89996326        0.001262 0.000000        1.111156\n#> 16    16               1.00000000        0.000075 0.000000        1.000000\n#>    response_rate    score cumulative_response_rate cumulative_score\n#> 1       1.000000 0.973603                 1.000000         0.973603\n#> 2       0.994764 0.945134                 0.997382         0.959368\n#> 3       1.000000 0.917635                 0.998252         0.945506\n#> 4       0.994764 0.888063                 0.997379         0.931126\n#> 5       0.984211 0.858560                 0.994753         0.916659\n#> 6       0.836306 0.717245                 0.915530         0.816952\n#> 7       0.381303 0.393216                 0.737579         0.675805\n#> 8       0.104932 0.175356                 0.579375         0.550660\n#> 9       0.025197 0.065862                 0.394682         0.389089\n#> 10      0.005249 0.026779                 0.297336         0.298524\n#> 11      0.000000 0.014450                 0.237850         0.241691\n#> 12      0.000000 0.009129                 0.198216         0.202937\n#> 13      0.000000 0.005802                 0.169903         0.174780\n#> 14      0.000000 0.003455                 0.148668         0.153367\n#> 15      0.000000 0.001882                 0.132151         0.136537\n#> 16      0.000000 0.000758                 0.118931         0.122954\n#>    capture_rate cumulative_capture_rate        gain cumulative_gain\n#> 1      0.084289                0.084289  740.820830      740.820830\n#> 2      0.083848                0.168138  736.418626      738.619728\n#> 3      0.083848                0.251986  740.820830      739.350863\n#> 4      0.083848                0.335834  736.418626      738.616843\n#> 5      0.082524                0.418358  727.544711      736.409388\n#> 6      0.351721                0.770079  603.183842      669.796615\n#> 7      0.160194                0.930274  220.607102      520.171557\n#> 8      0.044131                0.974404  -11.771162      387.150982\n#> 9      0.021183                0.995587  -78.813963      231.856507\n#> 10     0.004413                1.000000  -95.586242      150.006561\n#> 11     0.000000                1.000000 -100.000000       99.989504\n#> 12     0.000000                1.000000 -100.000000       66.663751\n#> 13     0.000000                1.000000 -100.000000       42.858214\n#> 14     0.000000                1.000000 -100.000000       25.003280\n#> 15     0.000000                1.000000 -100.000000       11.115647\n#> 16     0.000000                1.000000 -100.000000        0.000000\n#>    kolmogorov_smirnov\n#> 1            0.084289\n#> 2            0.168078\n#> 3            0.251926\n#> 4            0.335715\n#> 5            0.418060\n#> 6            0.760489\n#> 7            0.885596\n#> 8            0.878914\n#> 9            0.789475\n#> 10           0.681003\n#> 11           0.567463\n#> 12           0.453982\n#> 13           0.340502\n#> 14           0.227021\n#> 15           0.113540\n#> 16           0.000000\n#> \n#> $residual_deviance\n#> [1] 3749.845\n#> \n#> $null_deviance\n#> [1] 13900.71\n#> \n#> $AIC\n#> [1] 3763.845\n#> \n#> $null_degrees_of_freedom\n#> [1] 19052\n#> \n#> $residual_degrees_of_freedom\n#> [1] 19046\n\n##----.Precision vs Recall Plot----\nperformance_tbl <- performance_h2o %>%\n  h2o.metric() %>%\n  as.tibble() \n\n#> Warning: `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\nperformance_tbl %>% \n  glimpse()\n\n#> Rows: 400\n#> Columns: 20\n#> $ threshold               <dbl> 0.9917728, 0.9861981, 0.9831817, 0.9811547, 0.…\n#> $ f1                      <dbl> 0.009661836, 0.020960699, 0.026991728, 0.04063…\n#> $ f2                      <dbl> 0.006060606, 0.013204225, 0.017042331, 0.02579…\n#> $ f0point5                <dbl> 0.02380952, 0.05080440, 0.06485356, 0.09576202…\n#> $ accuracy                <dbl> 0.8816459, 0.8823282, 0.8826956, 0.8835354, 0.…\n#> $ precision               <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ recall                  <dbl> 0.004854369, 0.010591350, 0.013680494, 0.02074…\n#> $ specificity             <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ absolute_mcc            <dbl> 0.06541791, 0.09666166, 0.10987765, 0.13535059…\n#> $ min_per_class_accuracy  <dbl> 0.004854369, 0.010591350, 0.013680494, 0.02074…\n#> $ mean_per_class_accuracy <dbl> 0.5024272, 0.5052957, 0.5068402, 0.5103707, 0.…\n#> $ tns                     <dbl> 16787, 16787, 16787, 16787, 16787, 16787, 1678…\n#> $ fns                     <dbl> 2255, 2242, 2235, 2219, 2204, 2177, 2154, 2133…\n#> $ fps                     <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1…\n#> $ tps                     <dbl> 11, 24, 31, 47, 62, 89, 112, 133, 148, 164, 17…\n#> $ tnr                     <dbl> 1.0000000, 1.0000000, 1.0000000, 1.0000000, 1.…\n#> $ fnr                     <dbl> 0.9951456, 0.9894086, 0.9863195, 0.9792586, 0.…\n#> $ fpr                     <dbl> 0.000000e+00, 0.000000e+00, 0.000000e+00, 0.00…\n#> $ tpr                     <dbl> 0.004854369, 0.010591350, 0.013680494, 0.02074…\n#> $ idx                     <int> 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, …\n\ntheme_new <- theme(\n  legend.position  = \"bottom\",\n  legend.key       = element_blank(),\n  panel.background = element_rect(fill   = \"transparent\"),\n  panel.border     = element_rect(color = \"black\", fill = NA, size = 0.5),\n  panel.grid.major = element_line(color = \"grey\", size = 0.333)\n) \n\n#> Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\n\n#> Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 3.4.0.\n#> ℹ Please use the `linewidth` argument instead.\n\nperformance_tbl %>%\n  filter(f1 == max(f1))\n\n\n\n  \n\n\nperformance_tbl %>%\n  ggplot(aes(x = threshold)) +\n  geom_line(aes(y = precision), color = \"blue\", size = 1) +\n  geom_line(aes(y = recall), color = \"red\", size = 1) +\n  \n  # Insert line where precision and recall are harmonically optimized\n  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, \"f1\")) +\n  labs(title = \"Precision vs Recall\", y = \"value\") +\n  theme_new\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\n\n\n\n\n\n\n\n\n3 ROC Plot\n\n##----ROC Plot----\n##----.libraries----\nlibrary(h2o)\nlibrary(magrittr)\nlibrary(dplyr)\nlibrary(ggplot2)\nlibrary(tidyr)\nlibrary(tibble)\nlibrary(stringr)\nlibrary(forcats)\nlibrary(purrr)\n\n##----.Initialize H2O----\n\nh2o.init()\n\n#>  Connection successful!\n#> \n#> R is connected to the H2O cluster: \n#>     H2O cluster uptime:         2 hours 28 minutes \n#>     H2O cluster timezone:       Europe/Berlin \n#>     H2O data parsing timezone:  UTC \n#>     H2O cluster version:        3.40.0.4 \n#>     H2O cluster version age:    1 month and 10 days \n#>     H2O cluster name:           H2O_started_from_R_mosta_iae628 \n#>     H2O cluster total nodes:    1 \n#>     H2O cluster total memory:   3.04 GB \n#>     H2O cluster total cores:    12 \n#>     H2O cluster allowed cores:  12 \n#>     H2O cluster healthy:        TRUE \n#>     H2O Connection ip:          localhost \n#>     H2O Connection port:        54321 \n#>     H2O Connection proxy:       NA \n#>     H2O Internal Security:      FALSE \n#>     R Version:                  R version 4.2.3 (2023-03-15 ucrt)\n\nh2o.removeAll()\n##----.Load the training, validation, and test datasets----\ntrain_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\nvalid_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\ntest_df <- h2o.importFile(path = \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/product_backorders.csv\")\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\n##----.the response and predictor variables----\nresponse <- \"went_on_backorder\"\npredictors <- setdiff(names(train_df), response)\n\n##----.AutoML----\nautoml_models_h2o <- h2o.automl(\n  x = predictors,\n  y = response,\n  training_frame = train_df,\n  validation_frame = valid_df,\n  max_runtime_secs = 60\n)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |=                                                                     |   2%\n#> 15:58:59.609: User specified a validation frame with cross-validation still enabled. Please note that the models will still be validated using cross-validation only, the validation frame will be used to provide purely informative validation metrics on the trained models.\n#> 15:58:59.609: AutoML: XGBoost is not available; skipping it.\n  |                                                                            \n  |====                                                                  |   5%\n  |                                                                            \n  |======                                                                |   9%\n  |                                                                            \n  |=========                                                             |  12%\n  |                                                                            \n  |===========                                                           |  16%\n  |                                                                            \n  |=============                                                         |  19%\n  |                                                                            \n  |================                                                      |  23%\n  |                                                                            \n  |==================                                                    |  26%\n  |                                                                            \n  |=====================                                                 |  30%\n  |                                                                            \n  |=======================                                               |  33%\n  |                                                                            \n  |==========================                                            |  37%\n  |                                                                            \n  |============================                                          |  40%\n  |                                                                            \n  |===============================                                       |  44%\n  |                                                                            \n  |=================================                                     |  47%\n  |                                                                            \n  |====================================                                  |  51%\n  |                                                                            \n  |======================================                                |  54%\n  |                                                                            \n  |=========================================                             |  58%\n  |                                                                            \n  |===========================================                           |  62%\n  |                                                                            \n  |==============================================                        |  65%\n  |                                                                            \n  |================================================                      |  69%\n  |                                                                            \n  |==================================================                    |  72%\n  |                                                                            \n  |=====================================================                 |  76%\n  |                                                                            \n  |========================================================              |  79%\n  |                                                                            \n  |==========================================================            |  83%\n  |                                                                            \n  |==============================================================        |  89%\n  |                                                                            \n  |=================================================================     |  92%\n  |                                                                            \n  |===================================================================   |  96%\n  |                                                                            \n  |======================================================================|  99%\n  |                                                                            \n  |======================================================================| 100%\n\n##----leaderboard----\nleaderboard <- automl_models_h2o@leaderboard\nprint(leaderboard)\n\n#>                                                   model_id       auc   logloss\n#> 1    StackedEnsemble_AllModels_2_AutoML_19_20230607_155859 0.9565473 0.1633634\n#> 2 StackedEnsemble_BestOfFamily_3_AutoML_19_20230607_155859 0.9560944 0.1649751\n#> 3    StackedEnsemble_AllModels_1_AutoML_19_20230607_155859 0.9554309 0.1645058\n#> 4 StackedEnsemble_BestOfFamily_2_AutoML_19_20230607_155859 0.9543664 0.1670992\n#> 5                          GBM_4_AutoML_19_20230607_155859 0.9535425 0.1684353\n#> 6                          GBM_3_AutoML_19_20230607_155859 0.9525653 0.1706248\n#>       aucpr mean_per_class_error      rmse        mse\n#> 1 0.7618974            0.1435940 0.2208137 0.04875869\n#> 2 0.7558526            0.1464339 0.2214962 0.04906057\n#> 3 0.7585429            0.1493377 0.2216581 0.04913230\n#> 4 0.7490684            0.1613052 0.2228872 0.04967868\n#> 5 0.7488034            0.1420172 0.2239975 0.05017487\n#> 6 0.7486651            0.1420415 0.2251100 0.05067452\n#> \n#> [16 rows x 7 columns]\n\n##----Extract the leader model----\nleader_model <- automl_models_h2o@leader\nleader_model\n\n#> Model Details:\n#> ==============\n#> \n#> H2OBinomialModel: stackedensemble\n#> Model ID:  StackedEnsemble_AllModels_2_AutoML_19_20230607_155859 \n#> Model Summary for Stacked Ensemble: \n#>                                          key            value\n#> 1                          Stacking strategy cross_validation\n#> 2       Number of base models (used / total)              5/9\n#> 3           # GBM base models (used / total)              3/5\n#> 4           # DRF base models (used / total)              1/2\n#> 5           # GLM base models (used / total)              0/1\n#> 6  # DeepLearning base models (used / total)              1/1\n#> 7                      Metalearner algorithm              GLM\n#> 8         Metalearner fold assignment scheme           Random\n#> 9                         Metalearner nfolds                5\n#> 10                   Metalearner fold_column               NA\n#> 11        Custom metalearner hyperparameters             None\n#> \n#> \n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on training data. **\n#> \n#> MSE:  0.02565165\n#> RMSE:  0.1601613\n#> LogLoss:  0.09451412\n#> Mean Per-Class Error:  0.07428542\n#> AUC:  0.9899118\n#> AUCPR:  0.9419843\n#> Gini:  0.9798236\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>          No  Yes    Error        Rate\n#> No     8708  175 0.019701   =175/8883\n#> Yes     154 1041 0.128870   =154/1195\n#> Totals 8862 1216 0.032645  =329/10078\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold       value idx\n#> 1                       max f1  0.407464    0.863542 184\n#> 2                       max f2  0.251160    0.892942 238\n#> 3                 max f0point5  0.590143    0.895730 134\n#> 4                 max accuracy  0.496383    0.967454 158\n#> 5                max precision  0.994738    1.000000   0\n#> 6                   max recall  0.018085    1.000000 363\n#> 7              max specificity  0.994738    1.000000   0\n#> 8             max absolute_mcc  0.407464    0.845045 184\n#> 9   max min_per_class_accuracy  0.231190    0.948666 244\n#> 10 max mean_per_class_accuracy  0.219120    0.949994 249\n#> 11                     max tns  0.994738 8883.000000   0\n#> 12                     max fns  0.994738 1193.000000   0\n#> 13                     max fps  0.000243 8883.000000 399\n#> 14                     max tps  0.018085 1195.000000 363\n#> 15                     max tnr  0.994738    1.000000   0\n#> 16                     max fnr  0.994738    0.998326   0\n#> 17                     max fpr  0.000243    1.000000 399\n#> 18                     max tpr  0.018085    1.000000 363\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on validation data. **\n#> \n#> MSE:  0.02588367\n#> RMSE:  0.160884\n#> LogLoss:  0.0954548\n#> Mean Per-Class Error:  0.07683039\n#> AUC:  0.9896518\n#> AUCPR:  0.9419006\n#> Gini:  0.9793036\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error        Rate\n#> No     16467  320 0.019062  =320/16787\n#> Yes      305 1961 0.134598   =305/2266\n#> Totals 16772 2281 0.032803  =625/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.409632     0.862547 180\n#> 2                       max f2  0.251818     0.888183 232\n#> 3                 max f0point5  0.590210     0.897557 132\n#> 4                 max accuracy  0.488523     0.967564 157\n#> 5                max precision  0.994798     1.000000   0\n#> 6                   max recall  0.018608     1.000000 363\n#> 7              max specificity  0.994798     1.000000   0\n#> 8             max absolute_mcc  0.409632     0.843929 180\n#> 9   max min_per_class_accuracy  0.215293     0.945196 247\n#> 10 max mean_per_class_accuracy  0.199161     0.946903 253\n#> 11                     max tns  0.994798 16787.000000   0\n#> 12                     max fns  0.994798  2262.000000   0\n#> 13                     max fps  0.000138 16787.000000 399\n#> 14                     max tps  0.018608  2266.000000 363\n#> 15                     max tnr  0.994798     1.000000   0\n#> 16                     max fnr  0.994798     0.998235   0\n#> 17                     max fpr  0.000138     1.000000 399\n#> 18                     max tpr  0.018608     1.000000 363\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> H2OBinomialMetrics: stackedensemble\n#> ** Reported on cross-validation data. **\n#> ** 5-fold cross-validation on training data (Metrics computed for combined holdout predictions) **\n#> \n#> MSE:  0.04875869\n#> RMSE:  0.2208137\n#> LogLoss:  0.1633634\n#> Mean Per-Class Error:  0.143594\n#> AUC:  0.9565473\n#> AUCPR:  0.7618974\n#> Gini:  0.9130946\n#> \n#> Confusion Matrix (vertical: actual; across: predicted) for F1-optimal threshold:\n#>           No  Yes    Error         Rate\n#> No     15959  828 0.049324   =828/16787\n#> Yes      539 1727 0.237864    =539/2266\n#> Totals 16498 2555 0.071747  =1367/19053\n#> \n#> Maximum Metrics: Maximum metrics at their respective thresholds\n#>                         metric threshold        value idx\n#> 1                       max f1  0.327328     0.716449 207\n#> 2                       max f2  0.126480     0.789105 287\n#> 3                 max f0point5  0.563850     0.740142 136\n#> 4                 max accuracy  0.515111     0.934656 150\n#> 5                max precision  0.992926     1.000000   0\n#> 6                   max recall  0.000527     1.000000 398\n#> 7              max specificity  0.992926     1.000000   0\n#> 8             max absolute_mcc  0.327328     0.677143 207\n#> 9   max min_per_class_accuracy  0.126480     0.893645 287\n#> 10 max mean_per_class_accuracy  0.092825     0.895506 305\n#> 11                     max tns  0.992926 16787.000000   0\n#> 12                     max fns  0.992926  2264.000000   0\n#> 13                     max fps  0.000177 16787.000000 399\n#> 14                     max tps  0.000527  2266.000000 398\n#> 15                     max tnr  0.992926     1.000000   0\n#> 16                     max fnr  0.992926     0.999117   0\n#> 17                     max fpr  0.000177     1.000000 399\n#> 18                     max tpr  0.000527     1.000000 398\n#> \n#> Gains/Lift Table: Extract with `h2o.gainsLift(<model>, <data>)` or `h2o.gainsLift(<model>, valid=<T/F>, xval=<T/F>)`\n#> Cross-Validation Metrics Summary: \n#>                 mean        sd cv_1_valid cv_2_valid cv_3_valid cv_4_valid\n#> accuracy    0.930782  0.002983   0.926969   0.930827   0.930860   0.929957\n#> auc         0.956697  0.003859   0.953186   0.952708   0.957457   0.958065\n#> err         0.069218  0.002983   0.073031   0.069173   0.069140   0.070043\n#> err_count 263.800000 14.498276 280.000000 276.000000 259.000000 260.000000\n#> f0point5    0.703491  0.017836   0.673428   0.709557   0.716452   0.701829\n#>           cv_5_valid\n#> accuracy    0.935296\n#> auc         0.962070\n#> err         0.064704\n#> err_count 244.000000\n#> f0point5    0.716186\n#> \n#> ---\n#>                          mean        sd  cv_1_valid  cv_2_valid  cv_3_valid\n#> precision            0.692494  0.022231    0.654832    0.707217    0.698706\n#> r2                   0.534226  0.029274    0.495907    0.518374    0.572674\n#> recall               0.752744  0.028931    0.759725    0.719078    0.797468\n#> residual_deviance 1244.520000 80.534310 1289.774000 1357.026500 1219.111700\n#> rmse                 0.220733  0.004870    0.225627    0.225154    0.217324\n#> specificity          0.954742  0.005380    0.948484    0.959579    0.950183\n#>                    cv_4_valid  cv_5_valid\n#> precision            0.691824    0.709890\n#> r2                   0.534794    0.549382\n#> recall               0.744921    0.742529\n#> residual_deviance 1208.620200 1148.067500\n#> rmse                 0.221118    0.214440\n#> specificity          0.955032    0.960432\n\n##----.Predict using the leader model----\npredictions <- h2o.predict(leader_model, newdata = test_df)\n\n#> \n  |                                                                            \n  |                                                                      |   0%\n  |                                                                            \n  |======================================================================| 100%\n\npredictions_tbl <- as_tibble(predictions)\npredictions_tbl\n\n\n\n  \n\n\n##----.Set the directory path to save the leader model-----\nsave_directory <- \"C:/Users/mosta/Desktop/Business Decisions with Machine Learning/3/leadermodel\"\n\n##----.Save the leader model----\nh2o.saveModel(leader_model, path = save_directory, force = TRUE)\n\n#> [1] \"C:\\\\Users\\\\mosta\\\\Desktop\\\\Business Decisions with Machine Learning\\\\3\\\\leadermodel\\\\StackedEnsemble_AllModels_2_AutoML_19_20230607_155859\"\n\n# Define a function to load a model and calculate performance metrics\nload_model_performance_metrics <- function(model_id, test_df) {\n  model <- h2o.getModel(model_id)\n  perf_h2o  <- h2o.performance(model, newdata = test_df) \n  \n  perf_h2o %>%\n    h2o.metric() %>%\n    as.tibble() %>%\n    mutate(auc = h2o.auc(perf_h2o)) %>%\n    select(tpr, fpr, auc, precision, recall)\n}\n\n# Get the ids of the top 3 models\ntop_models <- as.character(automl_models_h2o@leaderboard[1:3, \"model_id\"] %>% as.data.frame() %>% pull())\n\n# Load models and calculate performance metrics\nmodel_metrics_tbl <- tibble(model_id = top_models) %>%\n  mutate(metrics = map(model_id, load_model_performance_metrics, test_df)) %>%\n  unnest(cols = metrics)\n\n#> Warning: There was 1 warning in `mutate()`.\n#> ℹ In argument: `metrics = map(model_id, load_model_performance_metrics,\n#>   test_df)`.\n#> Caused by warning:\n#> ! `as.tibble()` was deprecated in tibble 2.0.0.\n#> ℹ Please use `as_tibble()` instead.\n#> ℹ The signature and semantics have changed, see `?as_tibble`.\n\nroc_plot <- model_metrics_tbl %>%\n  mutate(\n    auc = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(fpr, tpr, color = model_id, linetype = auc)) +\n  geom_line(size = 1) +\n  # Just for demonstration purposes\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dotted\") +\n  theme_minimal() +\n  theme(legend.direction = \"vertical\") +\n  labs(\n    title = \"ROC Plot\",\n    subtitle = \"Performance of Models\",\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    color = \"Model\",\n    linetype = \"AUC\"\n  )\n\n#> Warning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\n#> ℹ Please use `linewidth` instead.\n\nprint(roc_plot)\n\n\n\n\n\n\n\n\n4 Precision vs Recall Plot\n\n##----.Precision vs Recall plot----\n\n#plot\nmodel_metrics_tbl %>%\n  mutate(\n    auc = auc %>% round(3) %>% as.character() %>% as_factor()\n  ) %>%\n  ggplot(aes(recall, precision, color = model_id, linetype = auc)) +\n  geom_line(size = 1) +\n  # Just for demonstration purposes\n  geom_abline(intercept = 0, slope = 1, color = \"red\", linetype = \"dotted\") +\n  theme_minimal() +\n  theme(legend.direction = \"vertical\") +\n  labs(\n    title = \"Precision vs Recall plot\",\n    subtitle = \"Performance of Models\",\n    x = \"False Positive Rate (FPR)\",\n    y = \"True Positive Rate (TPR)\",\n    color = \"Model\",\n    linetype = \"AUC\"\n  )\n\n\n\n\n\n\n\n\n5 Gain Plot\n\n##----.Gain & Lift----\nranked_predictions_tbl <- predictions_tbl %>%\n  bind_cols(as_tibble(test_df)) %>%\n  select(predict:Yes, went_on_backorder) %>%\n  # Sorting from highest to lowest class probability\n  arrange(desc(Yes)) %>%\n  print()\n\n#> # A tibble: 19,053 × 4\n#>    predict      No   Yes went_on_backorder\n#>    <fct>     <dbl> <dbl> <fct>            \n#>  1 Yes     0.00497 0.995 Yes              \n#>  2 Yes     0.00513 0.995 Yes              \n#>  3 Yes     0.00515 0.995 Yes              \n#>  4 Yes     0.00556 0.994 Yes              \n#>  5 Yes     0.00824 0.992 Yes              \n#>  6 Yes     0.00825 0.992 Yes              \n#>  7 Yes     0.00837 0.992 Yes              \n#>  8 Yes     0.00838 0.992 Yes              \n#>  9 Yes     0.00902 0.991 Yes              \n#> 10 Yes     0.00974 0.990 Yes              \n#> # ℹ 19,043 more rows\n\nranked_predictions_tbl %>%\n  mutate(ntile = ntile(Yes, n = 10)) %>%\n  group_by(ntile) %>%\n  summarise(\n    cases = n(),\n    responses = sum(went_on_backorder == \"Yes\")\n  ) %>%\n  arrange(desc(ntile))\n\n\n\n  \n\n\ncalculated_gain_lift_tbl <- ranked_predictions_tbl %>%\n  mutate(ntile = ntile(Yes, n = 10)) %>%\n  group_by(ntile) %>%\n  summarise(\n    cases = n(),\n    responses = sum(went_on_backorder == \"Yes\")\n  ) %>%\n  arrange(desc(ntile)) %>%\n  \n  # Add group numbers (opposite of ntile)\n  mutate(group = row_number()) %>%\n  select(group, cases, responses) %>%\n  \n  # Calculations\n  mutate(\n    cumulative_responses = cumsum(responses),\n    pct_responses        = responses / sum(responses),\n    gain                 = cumsum(pct_responses),\n    cumulative_pct_cases = cumsum(cases) / sum(cases),\n    lift                 = gain / cumulative_pct_cases,\n    gain_baseline        = cumulative_pct_cases,\n    lift_baseline        = gain_baseline / cumulative_pct_cases\n  )\n\ncalculated_gain_lift_tbl \n\n\n\n  \n\n\ngain_lift_tbl <- performance_h2o %>%\n  h2o.gainsLift() %>%\n  as.tibble()\n\n#Gain Plot\n\ngain_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"lift\")) %>%\n  mutate(baseline = cumulative_data_fraction) %>%\n  rename(gain     = cumulative_capture_rate) %>%\n  # prepare the data for the plotting (for the color and group aesthetics)\n  pivot_longer(cols = c(gain, baseline), values_to = \"value\", names_to = \"key\")\n\ngain_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Gain Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Gain\"\n  ) +\n  theme_new\n\n\n\n\n\n\n\n\n6 Lift Plot\n\n#Lift Plot\n\nlift_transformed_tbl <- gain_lift_tbl %>% \n  select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%\n  select(-contains(\"capture\")) %>%\n  mutate(baseline = 1) %>%\n  rename(lift = cumulative_lift) %>%\n  pivot_longer(cols = c(lift, baseline), values_to = \"value\", names_to = \"key\")\n\nlift_transformed_tbl %>%\n  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +\n  geom_line(size = 1.5) +\n  labs(\n    title = \"Lift Chart\",\n    x = \"Cumulative Data Fraction\",\n    y = \"Lift\"\n  ) +\n  theme_new\n\n\n\n\n\n\n\n\n7 Dashboard with cowplot\n\n#dashboard\nlibrary(cowplot)\nlibrary(glue)\n\n# Define the plot_h2o_performance function\nplot_h2o_performance <- function(h2o_leaderboard, newdata, order_by = c(\"auc\", \"logloss\"),\n                                 max_models = 3, size = 1.5) {\n  leaderboard_tbl <- h2o_leaderboard %>%\n    as_tibble() %>%\n    slice(1:max_models)\n  \n  newdata_tbl <- newdata %>%\n    as_tibble()\n  \n  order_by <- tolower(order_by[[1]])\n  order_by_expr <- rlang::sym(order_by)\n  \n  h2o.no_progress()\n  \n  get_model_performance_metrics <- function(model_id, test_tbl) {\n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl))\n    \n    perf_h2o %>%\n      h2o.metric() %>%\n      as.tibble() %>%\n      select(threshold, tpr, fpr, precision, recall)\n  }\n  \n  model_metrics_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_model_performance_metrics, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc      = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss  = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    )\n  \n  p1 <- model_metrics_tbl %>%\n    ggplot(aes(fpr, tpr, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_minimal() +\n    labs(title = \"ROC\", x = \"FPR\", y = \"TPR\") +\n    theme(legend.direction = \"vertical\") \n  \n  p2 <- model_metrics_tbl %>%\n    ggplot(aes(recall, precision, color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    theme_minimal() +\n    labs(title = \"Precision Vs Recall\", x = \"Recall\", y = \"Precision\") +\n    theme(legend.position = \"none\") \n  \n  get_gain_lift <- function(model_id, test_tbl) {\n    model_h2o <- h2o.getModel(model_id)\n    perf_h2o  <- h2o.performance(model_h2o, newdata = as.h2o(test_tbl)) \n    \n    perf_h2o %>%\n      h2o.gainsLift() %>%\n      as.tibble() %>%\n      select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift)\n  }\n  \n  gain_lift_tbl <- leaderboard_tbl %>%\n    mutate(metrics = map(model_id, get_gain_lift, newdata_tbl)) %>%\n    unnest(cols = metrics) %>%\n    mutate(\n      model_id = as_factor(model_id) %>% \n        fct_reorder(!! order_by_expr, \n                    .desc = ifelse(order_by == \"auc\", TRUE, FALSE)),\n      auc  = auc %>% \n        round(3) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id)),\n      logloss = logloss %>% \n        round(4) %>% \n        as.character() %>% \n        as_factor() %>% \n        fct_reorder(as.numeric(model_id))\n    ) %>%\n    rename(\n      gain = cumulative_capture_rate,\n      lift = cumulative_lift\n    ) \n  \n  p3 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, gain, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    geom_segment(x = 0, y = 0, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_minimal() +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Gain\", x = \"Cumulative Data Fraction\", y = \"Gain\") +\n    theme(legend.position = \"none\")\n  \n  p4 <- gain_lift_tbl %>%\n    ggplot(aes(cumulative_data_fraction, lift, \n               color = model_id, linetype = !! order_by_expr)) +\n    geom_line(size = size) +\n    geom_segment(x = 0, y = 1, xend = 1, yend = 1, \n                 color = \"red\", size = size, linetype = \"dotted\") +\n    theme_minimal() +\n    expand_limits(x = c(0, 1), y = c(0, 1)) +\n    labs(title = \"Lift\", x = \"Cumulative Data Fraction\", y = \"Lift\") +\n    theme(legend.position = \"none\") \n  \n  p_legend <- get_legend(p1)\n  p1 <- p1 + theme(legend.position = \"none\")\n  \n  p <- plot_grid(p1, p2, p3, p4, ncol = 2)\n  \n  p_title <- ggdraw() + \n    draw_label(\"H2O Model Metrics\", size = 18, fontface = \"bold\", \n               color = \"#2C3E50\")\n  \n  p_subtitle <- ggdraw() + \n    draw_label(glue(\"Ordered by {toupper(order_by)}\"), size = 10,  \n               color = \"#2C3E50\")\n  \n  ret <- plot_grid(p_title, p_subtitle, p, p_legend, \n                   ncol = 1, rel_heights = c(0.05, 0.05, 1, 0.05 * max_models))\n  \n  h2o.show_progress()\n  \n  return(ret)\n}\n\n##----.Generate the performance visualization dashboard----\nautoml_models_h2o@leaderboard %>%\n  plot_h2o_performance(newdata = test_df, order_by = \"logloss\", size = 0.5, max_models = 4)"
  },
  {
    "objectID": "content/01_journal/06_Explaining_Black-Box_Models_With_LIMEs.html",
    "href": "content/01_journal/06_Explaining_Black-Box_Models_With_LIMEs.html",
    "title": "Data Visualization",
    "section": "",
    "text": "Note\n\n\n\nYou can delete everything in here and start fresh."
  },
  {
    "objectID": "content/02_notes/05_class_notes.html",
    "href": "content/02_notes/05_class_notes.html",
    "title": "Class Notes",
    "section": "",
    "text": "IMPORTANT: You can delete everything in here and start fresh. You might want to start by not deleting anything above this line until you know what that stuff is doing.\nThis is an .qmd file. It is plain text with special features. Any time you write just like this, it will be compiled to normal text in the website. If you put a # in front of your text, it will create a top level-header."
  },
  {
    "objectID": "content/03_other/06_links.html",
    "href": "content/03_other/06_links.html",
    "title": "Links",
    "section": "",
    "text": "R is a free open-source programming language that can be used for statistical analysis, data-simulation, graphing, and lots of other stuff. Another free program is R-studio, that provides a nice graphic interface for R. Download R first, then download R-studio. Both can run on PCs, Macs or Linux. Students will be learning R in the stats labs using the lab manual .\n\n\n\n\nGoogle is great, Google your problem\nStackoverflow is great, google will often take you there because someone has already asked your question, and someone else has answered, usually many people have answered your question many ways."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "My Lab Journal",
    "section": "",
    "text": "Hi! In this website you will see the codes for the course “Business Decisions with Machine Learning”! :)"
  }
]